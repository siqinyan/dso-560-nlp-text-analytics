{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spacy\" data-toc-modified-id=\"Spacy-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Spacy</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Models</a></span></li></ul></li><li><span><a href=\"#Pattern-Matching-Using-Spacy\" data-toc-modified-id=\"Pattern-Matching-Using-Spacy-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Pattern Matching Using Spacy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-the-Target-Pattern\" data-toc-modified-id=\"Define-the-Target-Pattern-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Define the Target Pattern</a></span></li><li><span><a href=\"#Load-the-Pattern-into-the-Matcher\" data-toc-modified-id=\"Load-the-Pattern-into-the-Matcher-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Load the Pattern into the Matcher</a></span></li></ul></li><li><span><a href=\"#Using-Regular-Expressions-in-Spacy\" data-toc-modified-id=\"Using-Regular-Expressions-in-Spacy-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Using Regular Expressions in Spacy</a></span></li><li><span><a href=\"#Part-of-Speech-Tagging\" data-toc-modified-id=\"Part-of-Speech-Tagging-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Part of Speech Tagging</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Legend-for-spaCy-outputs-(https://spacy.io/usage/linguistic-features):\" data-toc-modified-id=\"Legend-for-spaCy-outputs-(https://spacy.io/usage/linguistic-features):-1.3.0.1\"><span class=\"toc-item-num\">1.3.0.1&nbsp;&nbsp;</span>Legend for <code>spaCy</code> outputs (<a href=\"https://spacy.io/usage/linguistic-features\" target=\"_blank\">https://spacy.io/usage/linguistic-features</a>):</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-of-Speech-Tagging\" data-toc-modified-id=\"Part-of-Speech-Tagging-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Part of Speech Tagging</a></span></li></ul></li><li><span><a href=\"#Hidden-Markov-Models\" data-toc-modified-id=\"Hidden-Markov-Models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hidden Markov Models</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Examples\" data-toc-modified-id=\"Examples-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Examples</a></span><ul class=\"toc-item\"><li><span><a href=\"#$V$\" data-toc-modified-id=\"$V$-2.0.1.1\"><span class=\"toc-item-num\">2.0.1.1&nbsp;&nbsp;</span>$V$</a></span></li><li><span><a href=\"#$O$\" data-toc-modified-id=\"$O$-2.0.1.2\"><span class=\"toc-item-num\">2.0.1.2&nbsp;&nbsp;</span>$O$</a></span></li><li><span><a href=\"#$A$\" data-toc-modified-id=\"$A$-2.0.1.3\"><span class=\"toc-item-num\">2.0.1.3&nbsp;&nbsp;</span>$A$</a></span></li><li><span><a href=\"#$B$\" data-toc-modified-id=\"$B$-2.0.1.4\"><span class=\"toc-item-num\">2.0.1.4&nbsp;&nbsp;</span>$B$</a></span></li><li><span><a href=\"#$\\pi$\" data-toc-modified-id=\"$\\pi$-2.0.1.5\"><span class=\"toc-item-num\">2.0.1.5&nbsp;&nbsp;</span>$\\pi$</a></span></li></ul></li></ul></li><li><span><a href=\"#Using-Hidden-Markov-Models-for-Tagging\" data-toc-modified-id=\"Using-Hidden-Markov-Models-for-Tagging-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Using Hidden Markov Models for Tagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-(15-minutes)\" data-toc-modified-id=\"Exercise-(15-minutes)-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Exercise (15 minutes)</a></span></li><li><span><a href=\"#Named-Entity-Recognition\" data-toc-modified-id=\"Named-Entity-Recognition-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Named Entity Recognition</a></span></li></ul></li><li><span><a href=\"#Merging-and-Splitting\" data-toc-modified-id=\"Merging-and-Splitting-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Merging and Splitting</a></span></li></ul></li><li><span><a href=\"#Word-Embeddings-(word2vec-Introduction)-from-Intro-to-Algorithmic-Marketing\" data-toc-modified-id=\"Word-Embeddings-(word2vec-Introduction)-from-Intro-to-Algorithmic-Marketing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word Embeddings (word2vec Introduction) from <strong>Intro to Algorithmic Marketing</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Continuous-Bag-of-Words-(Use-Context-to-Predict-Target-Word)\" data-toc-modified-id=\"Continuous-Bag-of-Words-(Use-Context-to-Predict-Target-Word)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Continuous Bag of Words (Use Context to Predict Target Word)</a></span></li><li><span><a href=\"#Softmax\" data-toc-modified-id=\"Softmax-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Softmax</a></span></li><li><span><a href=\"#Skipgram\" data-toc-modified-id=\"Skipgram-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Skipgram</a></span></li><li><span><a href=\"#Clusters-pf-Words\" data-toc-modified-id=\"Clusters-pf-Words-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Clusters pf Words</a></span></li></ul></li><li><span><a href=\"#Finding-Most-Similar-Words-(Using-Our-Old-Methods)\" data-toc-modified-id=\"Finding-Most-Similar-Words-(Using-Our-Old-Methods)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Finding Most Similar Words (Using Our Old Methods)</a></span></li><li><span><a href=\"#Exercise:-Similar-Words-Using-Word-Embeddings\" data-toc-modified-id=\"Exercise:-Similar-Words-Using-Word-Embeddings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exercise: Similar Words Using Word Embeddings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Spacy comes with a variety of different models that can used per language. For instance, the models for English are available [here](https://spacy.io/models/en). You'll need to download each model separately:\n",
    "\n",
    "```python\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download en_core_web_md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching Using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code and example is from Ashiq KS's article [Rule-Based Matching with spacy](https://medium.com/@ashiqgiga07/rule-based-matching-with-spacy-295b76ca2b68):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input text string is converted to a Document object\n",
    "text = '''\n",
    "Computer programming is the process of writing instructions that get executed by computers. \n",
    "The instructions, also known as code, are written in a programming language which the computer \n",
    "can understand and use to perform a task or solve a problem. Basic computer programming involves \n",
    "the analysis of a problem and development of a logical sequence of instructions to solve it. \n",
    "There can be numerous paths to a solution and the computer programmer seeks to design and \n",
    "code that which is most efficient. Among the programmer’s tasks are understanding requirements, \n",
    "determining the right programming language to use, designing or architecting the solution, coding, \n",
    "testing, debugging and writing documentation so that the solution can be easily\n",
    "understood by other programmers.Computer programming is at the heart of computer science. It is the \n",
    "implementation portion of software development, application development \n",
    "and software engineering efforts, transforming ideas and theories into actual, working solutions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher #import Matcher class from spacy\n",
    "#import the Span class to extract the words from the document object\n",
    "from spacy.tokens import Span \n",
    "\n",
    "#Language class with the English model 'en_core_web_sm' is loaded\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text) # convert the string above to a document\n",
    "\n",
    "#instantiate a new Matcher class object \n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Target Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pattern` object that you define should be a list of dictionary elements, each dictionary describing the token to match for. \n",
    "\n",
    "Here, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the pattern\n",
    "pattern = [{'LOWER': 'computer', 'POS': 'NOUN'},\n",
    "             {'POS':{'NOT_IN': ['VERB']}}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pattern into the Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the pattern to the previously created matcher object\n",
    "matcher.add(\"Matching\", None, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Regular Expressions in Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example can be found at https://spacy.io/usage/rule-based-matching. It uses the `re.finditer()` function to\n",
    "quickly iterate through all the matches found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def analyze_text(text):\n",
    "    rows = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        rows.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                     token.shape_, token.is_alpha, token.is_stop))\n",
    "    data = pd.DataFrame(rows, columns=[\"text\", \"lemma\", \"part_of_speech\", \"tag\", \"dependency\", \"shape\", \"is_alphanumeric\", \"is_stopword\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>tag</th>\n",
       "      <th>dependency</th>\n",
       "      <th>shape</th>\n",
       "      <th>is_alphanumeric</th>\n",
       "      <th>is_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steve</td>\n",
       "      <td>Steve</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jobs</td>\n",
       "      <td>Jobs</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>conj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>aux</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>buying</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>pcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>U.K.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>X.X.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>startup</td>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>compound</td>\n",
       "      <td>d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>billion</td>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma part_of_speech  tag dependency  shape  is_alphanumeric  \\\n",
       "0     Steve    Steve          PROPN  NNP   compound  Xxxxx             True   \n",
       "1      Jobs     Jobs          PROPN  NNP      nsubj   Xxxx             True   \n",
       "2       and      and          CCONJ   CC         cc    xxx             True   \n",
       "3     Apple    Apple          PROPN  NNP       conj  Xxxxx             True   \n",
       "4        is       be            AUX  VBZ        aux     xx             True   \n",
       "5   looking     look           VERB  VBG       ROOT   xxxx             True   \n",
       "6        at       at            ADP   IN       prep     xx             True   \n",
       "7    buying      buy           VERB  VBG      pcomp   xxxx             True   \n",
       "8      U.K.     U.K.          PROPN  NNP   compound   X.X.            False   \n",
       "9   startup  startup           NOUN   NN       dobj   xxxx             True   \n",
       "10      for      for            ADP   IN       prep    xxx             True   \n",
       "11        $        $            SYM    $   quantmod      $            False   \n",
       "12        1        1            NUM   CD   compound      d            False   \n",
       "13  billion  billion            NUM   CD       pobj   xxxx             True   \n",
       "\n",
       "    is_stopword  \n",
       "0         False  \n",
       "1         False  \n",
       "2          True  \n",
       "3         False  \n",
       "4          True  \n",
       "5         False  \n",
       "6          True  \n",
       "7         False  \n",
       "8         False  \n",
       "9         False  \n",
       "10         True  \n",
       "11        False  \n",
       "12        False  \n",
       "13        False  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_text(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend for `spaCy` outputs (https://spacy.io/usage/linguistic-features):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Text`: The original word text.\n",
    "* `Lemma`: The base form of the word.\n",
    "* `POS`: The simple part-of-speech tag.\n",
    "* `Tag`: The detailed part-of-speech tag.\n",
    "* `Dep`: Syntactic dependency, i.e. the relation between tokens.\n",
    "* `Shape`: The word shape – capitalization, punctuation, digits.\n",
    "* `is alpha`: Is the token an alpha character?\n",
    "* `is stop`: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_text(u\"Many jobs will come from this restaurant, which features very exotic eats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "How is spaCy able to tag the part of speech of a particular token? From its own documentation:\n",
    "\n",
    "> *A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.*\n",
    "\n",
    "\n",
    "There are a wide variety of statistical implementations for POS taggers, but a common one that is used is a **Hidden Markov Model**.\n",
    "\n",
    "Hidden Markov models allow us to work with both observed events and hidden events. \n",
    "\n",
    "# Hidden Markov Models\n",
    "\n",
    "An HMM is used frequently when we have to find the probability of a sequence of both **observable events**, and **hidden states**. \n",
    "\n",
    "[Speech and Language Processing, Jurafsky](https://web.stanford.edu/~jurafsky/slp3/):\n",
    "![hmm](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/hmm.png)\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### $V$\n",
    "\n",
    "$V$ is your vocabulary (if it is the entire English language, ~20,000 words)\n",
    "\n",
    "#### $O$\n",
    "\n",
    "**O** stands for **observed sequence**. Such as `I will race home`.\n",
    "\n",
    "\n",
    "* $Q$: `V`, `N`, `ADJ`, `ADV`, etc.\n",
    "#### $A$\n",
    "\n",
    "$a_{v \\rightarrow n}$ = 0.32, $a_{v \\rightarrow adv}$ = 0.44. $A$ is a square transition matrix that if $N$ x $N$, where $N$ is the number of different states.\n",
    "\n",
    "![transitions](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/transition.png)\n",
    "\n",
    "#### $B$\n",
    "\n",
    "$b_{n}(\"saw\") = 0.004$\n",
    "\n",
    "$B$ answers the question - *if we were to generate a random from state $i$, what is the likelihood this word would be $x$?*\n",
    "\n",
    "\n",
    "The reason it is called an ** emission probability ** is because it is the probability that a hidden state \"emits\" an observed value (in our case, the likelihood that a `NOUN` emits `bicycle`).\n",
    "\n",
    "$B$ should be based on a table that is of shape $N \\times V$. Here is an example of this emissions probability table, from a YouTube video I pulled off the internet:\n",
    "\n",
    "![emissions](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/emissions.png)\n",
    "\n",
    "\n",
    "#### $\\pi$\n",
    "\n",
    "$\\pi$ is the prior probability - ie. the percentage of all occurences in the entire corpus that are verbs, nouns ,etc.\n",
    "\n",
    "$\\pi_{v} = 0.3$, $\\pi_{n} = 0.35$, $\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jurafsky](https://web.stanford.edu/~jurafsky/slp3/):\n",
    "![hmm_diagram](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/hmm_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Hidden Markov Models for Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HMM often uses an algorithm called **Viterbi algorithm** to dynamically calculate the most likely sequence of hidden states. \n",
    "\n",
    "A great example of how to use the Viterbi algorithm, including Python code implementation, is available [here](http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/).\n",
    "\n",
    "### Exercise (15 minutes)\n",
    "\n",
    "1. Use the `viterbi_algorithm.xlsx` notebook to calculate the most likely Part of Speech tags in the document `Jane will`.\n",
    "\n",
    "2. In real life, how would you find the `B` emission frequencies?\n",
    "\n",
    "3. In real life, how would you find the `A` transition probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the entity types in spaCy is available [here](https://spacy.io/api/annotation#named-entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this using displacy:\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = \"I live in New York\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "print(\"After:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (word2vec Introduction) from **Intro to Algorithmic Marketing**"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (Use Context to Predict Target Word)\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/word2vec_cbow.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/softmax.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/skipgram.png \"Logo Title Text 1\")\n",
    "\n",
    "## Clusters pf Words\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/wordembedding_cluster.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((couch, sofa), 0.8256182074546814),\n",
       " ((sofa, couch), 0.8256182074546814),\n",
       " ((France, Paris), 0.7916327714920044),\n",
       " ((Paris, France), 0.7916327714920044),\n",
       " ((China, Chinese), 0.6921188831329346),\n",
       " ((Chinese, China), 0.6921188831329346),\n",
       " ((Beijing, China), 0.6201534271240234),\n",
       " ((China, Beijing), 0.6201534271240234),\n",
       " ((sad, depressed), 0.5758042335510254),\n",
       " ((depressed, sad), 0.5758042335510254),\n",
       " ((Beijing, Chinese), 0.5661282539367676),\n",
       " ((Chinese, Beijing), 0.5661282539367676),\n",
       " ((technology, computer), 0.5283228158950806),\n",
       " ((computer, technology), 0.5283228158950806),\n",
       " ((China, France), 0.5105164051055908),\n",
       " ((France, China), 0.5105164051055908),\n",
       " ((Beijing, Paris), 0.5006936192512512),\n",
       " ((Paris, Beijing), 0.5006936192512512),\n",
       " ((Mandarin, Chinese), 0.49335744976997375),\n",
       " ((Chinese, Mandarin), 0.49335744976997375),\n",
       " ((database, computer), 0.45259544253349304),\n",
       " ((computer, database), 0.45259544253349304),\n",
       " ((Beijing, France), 0.4522964656352997),\n",
       " ((France, Beijing), 0.4522964656352997),\n",
       " ((China, Paris), 0.4234945774078369),\n",
       " ((Paris, China), 0.4234945774078369),\n",
       " ((Chinese, France), 0.3995535373687744),\n",
       " ((France, Chinese), 0.3995535373687744),\n",
       " ((Chinese, Paris), 0.3940626084804535),\n",
       " ((Paris, Chinese), 0.3940626084804535),\n",
       " ((Mandarin, Beijing), 0.312273234128952),\n",
       " ((Beijing, Mandarin), 0.312273234128952),\n",
       " ((couch, computer), 0.30637744069099426),\n",
       " ((computer, couch), 0.30637744069099426),\n",
       " ((Mandarin, China), 0.2985212206840515),\n",
       " ((China, Mandarin), 0.2985212206840515),\n",
       " ((sad, couch), 0.2970384359359741),\n",
       " ((couch, sad), 0.2970384359359741),\n",
       " ((depressed, couch), 0.28867560625076294),\n",
       " ((couch, depressed), 0.28867560625076294),\n",
       " ((sad, Chinese), 0.2848646342754364),\n",
       " ((Chinese, sad), 0.2848646342754364),\n",
       " ((database, technology), 0.2792178690433502),\n",
       " ((technology, database), 0.2792178690433502),\n",
       " ((sofa, computer), 0.2545861303806305),\n",
       " ((computer, sofa), 0.2545861303806305),\n",
       " ((sofa, China), 0.25458353757858276),\n",
       " ((China, sofa), 0.25458353757858276),\n",
       " ((technology, China), 0.25161412358283997),\n",
       " ((China, technology), 0.25161412358283997),\n",
       " ((computer, Chinese), 0.2508227527141571),\n",
       " ((Chinese, computer), 0.2508227527141571),\n",
       " ((computer, China), 0.2347157895565033),\n",
       " ((China, computer), 0.2347157895565033),\n",
       " ((sofa, Paris), 0.22809794545173645),\n",
       " ((Paris, sofa), 0.22809794545173645),\n",
       " ((couch, Paris), 0.22482024133205414),\n",
       " ((Paris, couch), 0.22482024133205414),\n",
       " ((Mandarin, Paris), 0.221621572971344),\n",
       " ((Paris, Mandarin), 0.221621572971344),\n",
       " ((Mandarin, France), 0.2169257253408432),\n",
       " ((France, Mandarin), 0.2169257253408432),\n",
       " ((sofa, Chinese), 0.2066979557275772),\n",
       " ((Chinese, sofa), 0.2066979557275772),\n",
       " ((depressed, Chinese), 0.1925303190946579),\n",
       " ((Chinese, depressed), 0.1925303190946579),\n",
       " ((sad, computer), 0.18528936803340912),\n",
       " ((computer, sad), 0.18528936803340912),\n",
       " ((couch, Chinese), 0.18461398780345917),\n",
       " ((Chinese, couch), 0.18461398780345917),\n",
       " ((depressed, computer), 0.171308234333992),\n",
       " ((computer, depressed), 0.171308234333992),\n",
       " ((sad, China), 0.1702520102262497),\n",
       " ((China, sad), 0.1702520102262497),\n",
       " ((sad, Paris), 0.1698368340730667),\n",
       " ((Paris, sad), 0.1698368340730667),\n",
       " ((sofa, France), 0.168022021651268),\n",
       " ((France, sofa), 0.168022021651268),\n",
       " ((technology, Chinese), 0.1591387391090393),\n",
       " ((Chinese, technology), 0.1591387391090393),\n",
       " ((couch, China), 0.1554173231124878),\n",
       " ((China, couch), 0.1554173231124878),\n",
       " ((depressed, sofa), 0.1511470079421997),\n",
       " ((sofa, depressed), 0.1511470079421997),\n",
       " ((sad, sofa), 0.14914444088935852),\n",
       " ((sofa, sad), 0.14914444088935852),\n",
       " ((technology, France), 0.13670670986175537),\n",
       " ((France, technology), 0.13670670986175537),\n",
       " ((sad, France), 0.13597367703914642),\n",
       " ((France, sad), 0.13597367703914642),\n",
       " ((computer, France), 0.13529279828071594),\n",
       " ((France, computer), 0.13529279828071594),\n",
       " ((couch, France), 0.12682345509529114),\n",
       " ((France, couch), 0.12682345509529114),\n",
       " ((database, Chinese), 0.12570075690746307),\n",
       " ((Chinese, database), 0.12570075690746307),\n",
       " ((computer, Paris), 0.1215098649263382),\n",
       " ((Paris, computer), 0.1215098649263382),\n",
       " ((depressed, Paris), 0.11403124779462814),\n",
       " ((Paris, depressed), 0.11403124779462814)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = nlp(u'database Mandarin technology Beijing sad depressed couch sofa computer China Chinese France Paris')\n",
    "counter = Counter()\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if token1 != token2:\n",
    "            counter[(token1, token2)] = 1 - cosine(token1.vector, token2.vector)\n",
    "counter.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Words (Using Our Old Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# inspect the default settings for CountVectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>adult</th>\n",
       "      <th>advertised</th>\n",
       "      <th>ago</th>\n",
       "      <th>air</th>\n",
       "      <th>amazon</th>\n",
       "      <th>apart</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  actual  actually  adult  advertised  ago  air  amazon  \\\n",
       "0     0           0       0         0      0           0    0    0       0   \n",
       "1     0           0       0         0      0           0    0    0       0   \n",
       "2     0           0       0         0      0           0    0    0       0   \n",
       "3     0           0       0         0      0           0    0    0       0   \n",
       "4     0           0       0         0      0           1    0    0       0   \n",
       "\n",
       "   apart  ...  working  works  worse  worst  worth  wouldn  wrong  year  \\\n",
       "0      0  ...        0      0      0      0      0       0      0     0   \n",
       "1      0  ...        0      0      0      0      0       0      0     0   \n",
       "2      0  ...        0      0      0      0      0       0      0     0   \n",
       "3      0  ...        0      0      0      0      0       0      0     0   \n",
       "4      0  ...        0      0      0      0      0       0      0     0   \n",
       "\n",
       "   years  zero  \n",
       "0      0     0  \n",
       "1      0     0  \n",
       "2      0     0  \n",
       "3      0     0  \n",
       "4      0     0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reviews = open(\"../datasets/poor_amazon_toy_reviews.txt\").readlines()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create similiarity matrix\n",
    "similarity_matrix = pd.DataFrame(cosine_similarity(data.T.values), \n",
    "             columns=vectorizer.get_feature_names(),\n",
    "                                 index=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack matrix into table\n",
    "similarity_table = similarity_matrix.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "similarity_table.columns = [\"word1\", \"word2\", \"similarity\"]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249500, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table = similarity_table[similarity_table[\"similarity\"] < 0.99]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144497</th>\n",
       "      <td>old</td>\n",
       "      <td>year</td>\n",
       "      <td>0.754569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194593</th>\n",
       "      <td>service</td>\n",
       "      <td>customer</td>\n",
       "      <td>0.734095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237276</th>\n",
       "      <td>waste</td>\n",
       "      <td>money</td>\n",
       "      <td>0.655483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245419</th>\n",
       "      <td>working</td>\n",
       "      <td>stopped</td>\n",
       "      <td>0.589414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>figure</td>\n",
       "      <td>figures</td>\n",
       "      <td>0.553856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>arm</td>\n",
       "      <td>train</td>\n",
       "      <td>0.500379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172826</th>\n",
       "      <td>quality</td>\n",
       "      <td>poor</td>\n",
       "      <td>0.469081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179083</th>\n",
       "      <td>remote</td>\n",
       "      <td>control</td>\n",
       "      <td>0.456930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210121</th>\n",
       "      <td>store</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.426935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>apart</td>\n",
       "      <td>fell</td>\n",
       "      <td>0.385922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  similarity\n",
       "144497      old      year    0.754569\n",
       "194593  service  customer    0.734095\n",
       "237276    waste     money    0.655483\n",
       "245419  working   stopped    0.589414\n",
       "74650    figure   figures    0.553856\n",
       "5949        arm     train    0.500379\n",
       "172826  quality      poor    0.469081\n",
       "179083   remote   control    0.456930\n",
       "210121    store    dollar    0.426935\n",
       "4648      apart      fell    0.385922"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table.sort_values(by=\"similarity\", ascending=False).drop_duplicates(\n",
    "    subset=\"similarity\", keep=\"first\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Similar Words Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into spacy your top 500 words\n",
    "\n",
    "tokens = nlp(f'{\" \".join(top_500_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create a list of similarity tuples\n",
    "\n",
    "similarity_tuples = []\n",
    "\n",
    "for token1, token2 in product(tokens, repeat=2):\n",
    "    similarity_tuples.append((token1, token2, token1.similarity(token2)))\n",
    "\n",
    "similarities = pd.DataFrame(similarity_tuples, columns=[\"word1\",\"word2\", \"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193151</th>\n",
       "      <td>seams</td>\n",
       "      <td>seam</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249618</th>\n",
       "      <td>wouldn</td>\n",
       "      <td>don</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103344</th>\n",
       "      <td>horrible</td>\n",
       "      <td>terrible</td>\n",
       "      <td>0.949510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102925</th>\n",
       "      <td>horrible</td>\n",
       "      <td>awful</td>\n",
       "      <td>0.918639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217883</th>\n",
       "      <td>terrible</td>\n",
       "      <td>awful</td>\n",
       "      <td>0.914612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28672</th>\n",
       "      <td>card</td>\n",
       "      <td>cards</td>\n",
       "      <td>0.908339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84503</th>\n",
       "      <td>games</td>\n",
       "      <td>game</td>\n",
       "      <td>0.904622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13077</th>\n",
       "      <td>battery</td>\n",
       "      <td>batteries</td>\n",
       "      <td>0.900290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227073</th>\n",
       "      <td>trash</td>\n",
       "      <td>garbage</td>\n",
       "      <td>0.893240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226349</th>\n",
       "      <td>toys</td>\n",
       "      <td>toy</td>\n",
       "      <td>0.889482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219607</th>\n",
       "      <td>think</td>\n",
       "      <td>know</td>\n",
       "      <td>0.885071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243453</th>\n",
       "      <td>wheel</td>\n",
       "      <td>wheels</td>\n",
       "      <td>0.884899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219560</th>\n",
       "      <td>think</td>\n",
       "      <td>guess</td>\n",
       "      <td>0.884596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>totally</td>\n",
       "      <td>completely</td>\n",
       "      <td>0.880790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93642</th>\n",
       "      <td>guess</td>\n",
       "      <td>maybe</td>\n",
       "      <td>0.880381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176807</th>\n",
       "      <td>really</td>\n",
       "      <td>definitely</td>\n",
       "      <td>0.880324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150903</th>\n",
       "      <td>pack</td>\n",
       "      <td>packs</td>\n",
       "      <td>0.879089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152918</th>\n",
       "      <td>paid</td>\n",
       "      <td>pay</td>\n",
       "      <td>0.878251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101607</th>\n",
       "      <td>hole</td>\n",
       "      <td>holes</td>\n",
       "      <td>0.877425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162970</th>\n",
       "      <td>playing</td>\n",
       "      <td>play</td>\n",
       "      <td>0.877274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1       word2     score\n",
       "193151     seams        seam  1.000000\n",
       "249618    wouldn         don  1.000000\n",
       "103344  horrible    terrible  0.949510\n",
       "102925  horrible       awful  0.918639\n",
       "217883  terrible       awful  0.914612\n",
       "28672       card       cards  0.908339\n",
       "84503      games        game  0.904622\n",
       "13077    battery   batteries  0.900290\n",
       "227073     trash     garbage  0.893240\n",
       "226349      toys         toy  0.889482\n",
       "219607     think        know  0.885071\n",
       "243453     wheel      wheels  0.884899\n",
       "219560     think       guess  0.884596\n",
       "224978   totally  completely  0.880790\n",
       "93642      guess       maybe  0.880381\n",
       "176807    really  definitely  0.880324\n",
       "150903      pack       packs  0.879089\n",
       "152918      paid         pay  0.878251\n",
       "101607      hole       holes  0.877425\n",
       "162970   playing        play  0.877274"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar words\n",
    "similarities[similarities[\"score\"] < 1].sort_values(\n",
    "    by=\"score\", ascending=False).drop_duplicates(\n",
    "    subset=\"score\", keep=\"first\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
