{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjElzEXdaO9-"
   },
   "source": [
    "### What is Textacy?\n",
    "\n",
    "[textacy](https://github.com/chartbeat-labs/textacy) is a text pre/post-processing framework that will help make many of the tasks we performed in this course significantly easier. As its [Github description](https://github.com/chartbeat-labs/textacy) states:\n",
    "> *textacy is a Python library for performing a variety of natural language processing (NLP) tasks, built on the high-performance spaCy library. With the fundamentals --- tokenization, part-of-speech tagging, dependency parsing, etc. --- delegated to another library, textacy focuses primarily on the tasks that come before and follow after.*\n",
    "\n",
    "While `spacy` focuses on tokenization, part of speech tagging, named entity recognition, etc., `textacy` focuses on all the different tasks that come before and after.\n",
    "\n",
    "Check out the [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/quickstart.html#) for all the different use cases you can apply `textacy` to - only a few common ones are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q43BRs2Tv7Fz"
   },
   "outputs": [],
   "source": [
    "# # install library\n",
    "# !pip install textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6e655U5aV0D"
   },
   "source": [
    "### Import Data\n",
    "We will import the `SMS_train.csv` dataset from week 4 homework to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:29:50.348305Z",
     "start_time": "2022-04-06T02:29:49.931510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxKtTwpCaTBv",
    "outputId": "3d5a0706-7cb3-4cca-d1f4-8b559d178ef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sms_df = pd.read_csv(\"../datasets/SMS_train.csv\", encoding=\"latin1\")\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:32:54.572592Z",
     "start_time": "2022-04-06T02:32:54.538185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>953</td>\n",
       "      <td>hows my favourite person today? r u workin har...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>954</td>\n",
       "      <td>How much you got for cleaning</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>955</td>\n",
       "      <td>Sorry da. I gone mad so many pending works wha...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>956</td>\n",
       "      <td>Wat time ü finish?</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>957</td>\n",
       "      <td>Just glad to be talking to you.</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     S. No.                                       Message_body     Label\n",
       "0         1                         Rofl. Its true to its name  Non-Spam\n",
       "1         2  The guy did some bitching but I acted like i'd...  Non-Spam\n",
       "2         3  Pity, * was in mood for that. So...any other s...  Non-Spam\n",
       "3         4               Will ü b going to esplanade fr home?  Non-Spam\n",
       "4         5  This is the 2nd time we have tried 2 contact u...      Spam\n",
       "..      ...                                                ...       ...\n",
       "952     953  hows my favourite person today? r u workin har...  Non-Spam\n",
       "953     954                      How much you got for cleaning  Non-Spam\n",
       "954     955  Sorry da. I gone mad so many pending works wha...  Non-Spam\n",
       "955     956                                 Wat time ü finish?  Non-Spam\n",
       "956     957                    Just glad to be talking to you.  Non-Spam\n",
       "\n",
       "[957 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS0m1HOTbn90"
   },
   "source": [
    "#### Grouping Concepts\n",
    "\n",
    "One of the attributes of this dataset is the presence of URLs. Textacy has already defined regex to parse out URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:33:55.537010Z",
     "start_time": "2022-04-06T02:33:42.470201Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPNLfPvda6O3",
    "outputId": "0e179fed-350f-4d9c-b76d-36d319b17796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex for URLs: re.compile('(?:^|(?<![\\\\w/.]))(?:(?:https?://|ftp://|www\\\\d{0,3}\\\\.))(?:\\\\S+(?::\\\\S*)?@)?(?:(?!(?:10|127)(?:\\\\.\\\\d{1,3}){3})(?!(?:169\\\\.254|192\\\\.168)(?:\\\\.\\\\d{1,3}){2})(?!172\\\\.(?:1[6-9]|2\\\\d|3[0-1])(?:\\\\.\\\\d{1, re.IGNORECASE)\n",
      "Regex for short URLs: re.compile('(?:^|(?<![\\\\w/.]))(?:(?:https?://)?)(?:\\\\w-?)*?\\\\w+(?:\\\\.[a-z]{2,12}){1,3}/[^\\\\s.,?!\\'\\\\\"|+]{2,12}(?:$|(?![\\\\w?!+&/]))', re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import itertools\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "from textacy.preprocessing.resources import RE_SHORT_URL\n",
    "print(f\"Regex for URLs: {RE_URL}\")\n",
    "print(f\"Regex for short URLs: {RE_SHORT_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "professional pre-defined patterns for URLs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:35:09.455000Z",
     "start_time": "2022-04-06T02:35:09.422375Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPNLfPvda6O3",
    "outputId": "0e179fed-350f-4d9c-b76d-36d319b17796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following URLs: ['www.comuk.net', 'www.gamb.tv', 'www.shortbreaks.org.uk', 'www.dbuk.net', 'www.t-c.biz', 'www.SMS.ac/u/nat27081980', 'www.telediscount.co.uk', 'www.getzed.co.uk', 'www.ringtones.co.uk', 'www.SMS.ac/u/natalie2k9', 'www.SMS.ac/u/goldviking', 'www.SMS.ac/u/hmmross', 'www.4-tc.biz', 'www.santacalling.com', 'www.fullonsms.com', 'www.cashbin.co.uk', 'www.win-82050.co.uk', 'www.clubmoby.com']\n"
     ]
    }
   ],
   "source": [
    "results: List[List[str]] = sms_df.Message_body.str.findall(RE_URL).tolist()\n",
    "\n",
    "parsed_urls: List[str] = list(itertools.chain(*results))\n",
    "print(f\"Found the following URLs: {parsed_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPKYZ2v_dU0M"
   },
   "source": [
    "We can quickly replace all of these URLs with a predefined tagged token, like `_URL_` by using the `replace_urls` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:35:27.192409Z",
     "start_time": "2022-04-06T02:35:27.178630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3ITF6LOTjF96",
    "outputId": "aa8a9bb9-c09a-45c5-f234-d0d36071d1a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a url: _URL_'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls\n",
    "text = \"This is a url: http://www.google.com\"\n",
    "urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:35:57.980053Z",
     "start_time": "2022-04-06T02:35:57.907210Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-VxdCtKbr8S",
    "outputId": "cf62b0b3-92f3-40ac-fad1-d114c99441a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried 2 contact u...\n",
       "Name: Message_body, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.Message_body.apply(urls)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPSKq_sTlzZ6"
   },
   "source": [
    "We can replace all sorts of different entities/concepts, such as URLs, hashtags, numbers, emails, etc.\n",
    "\n",
    "We can also use the regex defined by `textacy`. Below we define a pipeline to find and replace common entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:36:27.478487Z",
     "start_time": "2022-04-06T02:36:27.400565Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UohDlYzXdf-C",
    "outputId": "3d7ff9a5-a12f-4cc3-bd4d-cca49fb0ae0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried _NUMBER_ co...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls, hashtags, numbers, emails, emojis, currency_symbols\n",
    "sms_df[\"cleaned_text\"] = sms_df.Message_body.\\\n",
    "  apply(urls).\\\n",
    "  apply(hashtags).\\\n",
    "  apply(numbers).\\\n",
    "  apply(currency_symbols).\\\n",
    "  apply(emojis).\\\n",
    "  apply(emails)\n",
    "sms_df.cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjVspJFHmbHx"
   },
   "source": [
    "We can also use `textacy` to remove or normalized undesired text elements. For instance, there are often many different manifestations of quotation marks and bullet points, especially if you are dealing with text that is formatted from a word processor like Microsoft Word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:37:49.577846Z",
     "start_time": "2022-04-06T02:37:49.535504Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgilj4rznEmR",
    "outputId": "0fa3bebf-50c5-40f8-d349-e213fbc42827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before counts: Counter({'\"': 1, '“': 1, '”': 1})\n",
      "After counts: Counter({'\"': 3})\n",
      "Before counts: Counter({'•': 1, '‣': 1, '⁃': 1, '-': 1})\n",
      "Before counts: Counter({'-': 4})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from textacy.preprocessing.normalize import quotation_marks, bullet_points\n",
    "quotes = ['\"','“','”']\n",
    "print(f\"Before counts: {Counter(quotes)}\")\n",
    "print(f\"After counts: {Counter(map(quotation_marks, quotes))}\")\n",
    "\n",
    "points = [\"•\", \"‣\", \"⁃\", \"-\"]\n",
    "print(f\"Before counts: {Counter(points)}\")\n",
    "print(f\"Before counts: {Counter(map(bullet_points, points))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "normalize punctuations</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJJTce3Lo2Ow"
   },
   "source": [
    "A common text preprocessing task we performed in this course is removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:38:08.943762Z",
     "start_time": "2022-04-06T02:38:08.580480Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF0ComCrnU1G",
    "outputId": "572d6940-3b2f-41d3-c21f-19b95b926fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl  Its true to its name\n",
       "1    The guy did some bitching but I acted like i d...\n",
       "2    Pity    was in mood for that  So   any other s...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.remove import punctuation\n",
    "sms_df.cleaned_text[:3].apply(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bnmvij0pUSk"
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "You can also use `textacy` to extract ngrams, named entities, and even key terms from a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:38:44.967117Z",
     "start_time": "2022-04-06T02:38:40.727873Z"
    },
    "id": "2Y_ViMIvqYx3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 4.7 MB/s eta 0:00:01    |██▏                             | 942 kB 4.7 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/antheayang/opt/anaconda3/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "pretrained models, e.g. en_core_web_sm</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T17:46:28.769597Z",
     "start_time": "2022-04-06T17:46:27.844367Z"
    },
    "id": "50qEggFhp11A"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"\"\"\n",
    "I am eating dinner at the restaurant on Main Street, the best eatery this side of New York City. \n",
    "He went running down the street, but could not find his bike.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:40:15.914981Z",
     "start_time": "2022-04-06T02:40:15.894644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "I am eating dinner at the restaurant on Main Street, the best eatery this side of New York City. \n",
       "He went running down the street, but could not find his bike."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:40:08.987262Z",
     "start_time": "2022-04-06T02:40:08.959864Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03357045,  0.09447847,  0.0132584 ,  0.12589869,  0.22478124,\n",
       "       -0.20145367, -0.4253439 ,  0.12956926,  0.04941427,  0.13796641,\n",
       "       -0.02792004, -0.12966347,  0.0747843 , -0.19523801,  0.42932856,\n",
       "        0.0869386 ,  0.07878901,  0.1333638 ,  0.06589436,  0.01282445,\n",
       "        0.03202352, -0.47435796,  0.13670973,  0.18267448,  0.08760847,\n",
       "       -0.22268058, -0.53094196, -0.4760784 ,  0.07142697, -0.2580619 ,\n",
       "        0.1622928 , -0.28120098,  0.34055528,  0.23792443,  0.37506774,\n",
       "       -0.41053584, -0.26462954,  0.05119904, -0.10167839,  0.18882272,\n",
       "       -0.34467638, -0.15742545,  0.41096944, -0.12499912, -0.24822289,\n",
       "       -0.12835489, -0.15358229, -0.29547682,  0.29448593, -0.02160036,\n",
       "       -0.0617879 , -0.04666579,  0.31824395,  0.04851002,  0.19941165,\n",
       "       -0.17772223, -0.15905868,  0.02719001,  0.14229092, -0.30467686,\n",
       "       -0.23200154, -0.09337588,  0.05068191, -0.22481915, -0.01508204,\n",
       "        0.25666094,  0.12403066, -0.16099261,  0.10894626, -0.25761217,\n",
       "        0.2147762 ,  0.06567287, -0.00275069,  0.2800993 ,  0.08941555,\n",
       "       -0.18690696, -0.27150807, -0.00574527,  0.34627688, -0.2818276 ,\n",
       "        0.05112445,  0.1226786 ,  0.15869439,  0.16972344, -0.14350191,\n",
       "       -0.00591558, -0.12120755,  0.15241574, -0.22263269,  0.09579389,\n",
       "       -0.19519041,  0.26830032,  0.02661925,  0.13766922,  0.03573207,\n",
       "        0.21978459], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "<b>name entities:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:40:31.234482Z",
     "start_time": "2022-04-06T02:40:31.218405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Main Street, New York City)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T17:47:05.660072Z",
     "start_time": "2022-04-06T17:47:05.621733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Obama, the White House, first, Democrats)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nlp(\"\"\"\n",
    "Obama will return to the White House for the first time as Democrats look ahead to midterm elections\"\"\")\n",
    "test.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:41:17.524581Z",
     "start_time": "2022-04-06T02:41:17.513495Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veUW_C7CpTzP",
    "outputId": "8d80d477-8213-4c96-e16d-632f00bfa776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-grams with stopwords: [I am, am eating, eating dinner, dinner at, at the, the restaurant, restaurant on, on Main, Main Street, the best, best eatery, eatery this, this side, side of, of New, New York, York City, He went, went running, running down, down the, the street, but could, could not, not find, find his, his bike]\n",
      "n-grams without stopwords: [eating dinner, Main Street, best eatery, New York, York City, went running]\n"
     ]
    }
   ],
   "source": [
    "from textacy import extract\n",
    "# note that you must pass in a spacy Doc, not a string\n",
    "print(f\"n-grams with stopwords: {list(extract.ngrams(doc, n=2, filter_stops=False))}\")\n",
    "print(f\"n-grams without stopwords: {list(extract.ngrams(doc, n=2, filter_stops=True))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:41:50.218974Z",
     "start_time": "2022-04-06T02:41:50.196899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zSPBm2_pFD3",
    "outputId": "1559262a-7749-4701-fb51-ecdfd105ac20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named entities: [Main Street, New York City]\n"
     ]
    }
   ],
   "source": [
    "print(f\"named entities: {list(extract.entities(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "<p>name entities tell the topic, important signals. \n",
    "<p>Should be tokenized as one e.g. New York City -> _NEW_YORK_CITY_</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCjACFWsS7y"
   },
   "source": [
    "### Parsing Key Terms\n",
    "`textacy` also can attempt to parse out what it believes are key words from a particular document. There are a variety of algorithms it can use:\n",
    "\n",
    "* [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "* [SGRank](https://aclanthology.org/S15-1013.pdf)\n",
    "* [YAKE](https://github.com/LIAAD/yake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:43:06.372392Z",
     "start_time": "2022-04-06T02:43:06.356400Z"
    }
   },
   "source": [
    "<div class=\"alert-success\">\n",
    "<p>given on some scorings, tells important tokens\n",
    "<p> window size is important</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:44:19.895899Z",
     "start_time": "2022-04-06T02:44:19.844860Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qxW1O38rtKU",
    "outputId": "3d289268-57b4-4e1f-d2a3-4928431d1a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.08906773052656537), ('good eatery', 0.05593421627154432), ('Main Street', 0.05483321797094359), ('bike', 0.028799215152480313), ('dinner', 0.0285773930627672), ('restaurant', 0.026648908092068536), ('street', 0.02508976848714809)]\n",
      "key terms w/ window size = 4: [('New York City', 0.08858588611075899), ('good eatery', 0.05758818253165755), ('Main Street', 0.05327412352235519), ('dinner', 0.029984215962700136), ('restaurant', 0.029182573041746988), ('street', 0.028744547498104688), ('bike', 0.021793912595648182)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.textrank(doc))}\")\n",
    "print(f\"key terms w/ window size = 4: {list(extract.keyterms.textrank(doc, window_size=4))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:44:21.102330Z",
     "start_time": "2022-04-06T02:44:21.084522Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_pJUGWGsNQM",
    "outputId": "87d0030c-c3d8-436a-8634-6d5e48962417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.3517458042211082), ('good eatery', 0.2112484604816762), ('Main Street', 0.15856049498256797), ('restaurant', 0.08132137758569719), ('street', 0.06737092215444981), ('bike', 0.06561240483667229), ('dinner', 0.06414053573782827)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.sgrank(doc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:44:21.717026Z",
     "start_time": "2022-04-06T02:44:21.698250Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyDKOCEjH6uD",
    "outputId": "10198dd7-0bc6-4f6d-c3a7-eb13b0fe113e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.333801710490245), ('Main Street', 0.44164399917429203), ('bike', 0.7774388474035969), ('good', 0.8049257265599533), ('dinner', 0.8392874245523302), ('restaurant', 0.8392874245523302), ('eatery', 0.8392874245523302), ('street', 0.8613045009868965), ('good eatery', 2.08227238435987)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.yake(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQpFu9zpIqQf"
   },
   "source": [
    "### Generating Text Statistics\n",
    "\n",
    "You can often summarize a corpus and examine its properties to determine how similar one corpus is to another corpus. [Textacy has a number of functions to help parse out these properties/statistics](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html#textacy.text_stats.readability.gunning_fog_index). This can be useful for identifying authorship or source when you are not certain where certain text originated from, or if you wish to cluster text together using an unsupervised clustering algorithm such as **K-Nearest Neighbors**.\n",
    "\n",
    "Common useful stats (definitions directly from [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html)):\n",
    "- **[Flesch Reading Ease](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch.E2.80.93Kincaid_grade_level)**: Readability test used as a general-purpose standard in several languages, based on a weighted combination of avg. sentence length and avg. word length. Values usually fall in the range [0, 100], but may be arbitrarily negative in extreme cases. Higher value => easier text.\n",
    "- **[Gunning Fog Index](https://en.wikipedia.org/wiki/Gunning_fog_index)**: Readability test commonly used in Sweden on both English- and non-English-language texts, whose value estimates the difficulty of reading a foreign text. Higher value => more difficult text.\n",
    "- **[Smog Index](https://en.wikipedia.org/wiki/SMOG)**: Readability test commonly used in medical writing and the healthcare industry, whose value estimates the number of years of education required to understand a text similar to `flesch_kincaid_grade_level()` and intended as a substitute for `gunning_fog_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:46:35.664432Z",
     "start_time": "2022-04-06T02:46:34.722477Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVGB2jYgH8-j",
    "outputId": "be7df341-9c96-47a1-f6d5-acc54928c4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 6.345230909424329\n",
      "Flesch Grade Level: 8.509090909090908\n",
      "Smog Index: 10.504223727775692\n"
     ]
    }
   ],
   "source": [
    "from textacy.text_stats import TextStats\n",
    "from textacy import make_spacy_doc\n",
    "doc = make_spacy_doc(\"\"\"\n",
    "A month ago, new coronavirus cases in the United States were ticking steadily \n",
    "downward and the worst of a miserable summer surge fueled by the Delta variant \n",
    "appeared to be over. But as Americans travel this week to meet far-flung \n",
    "relatives for Thanksgiving dinner, new virus cases are rising once more, \n",
    "especially in the Upper Midwest and Northeast.\n",
    "\n",
    "Federal medical teams have been dispatched to Minnesota to help at overwhelmed \n",
    "hospitals. Michigan is enduring its worst case surge yet, with daily caseloads \n",
    "doubling since the start of November. Even New England, where vaccination rates \n",
    "are high, is struggling, with Vermont, Maine and New Hampshire trying to \n",
    "contain major outbreaks.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T02:46:41.794171Z",
     "start_time": "2022-04-06T02:46:41.770830Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPUK2xmrJJ1q",
    "outputId": "65f6a0e5-a244-45ed-e8f7-465beeec0620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.584962500721156\n",
      "Flesch Grade Level: -3.2049999999999983\n",
      "Smog Index: 3.1291\n"
     ]
    }
   ],
   "source": [
    "doc = make_spacy_doc(\"\"\"\n",
    "He do good.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:35:08.506140Z",
     "start_time": "2022-04-20T07:35:08.490990Z"
    }
   },
   "source": [
    "### CW for week4:\n",
    "\n",
    "1. Which pair of words from the list of four below would have the closest similarity score using word2vec? Explain why based on your understanding of word2vec. *happy, hoppy, cheerful, derecha (Spanish word for right)*\n",
    "- (happy, cheerful). As word2vec is based on the assumption that the meaning of a word is much affected by its context, it would return similar vectors for synonyms, which are used in similar context.\n",
    "\n",
    "2. Identify all the named entities in the following document:\n",
    "Obama will return to the White House for the first time as Democrats look ahead to midterm elections\n",
    "- Obama, the White House, Democrats, midterm elections.\n",
    "\n",
    "3. Write a named or unnamed capture group to extract email address' user names(the part before the @).\n",
    "- r'(\\w+)@\\w+\\.(?:com|net)'\n",
    "\n",
    "#### verProf\n",
    "2. midterm election may not be a named entity\n",
    "3. Don't forget word boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Using Textacy for Text Pre-Processing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
