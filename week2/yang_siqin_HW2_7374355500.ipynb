{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (Due 6:29pm PST March 29th, 2022): Word Vectorization, Regex Practice, and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may work with **one other person on this assignment**. You may also work independently if you prefer.\n",
    "\n",
    "If you just want to be assigned someone to work with, message me on Slack and I will assign you a partner to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This homework is co-completed by **Siqin Yang** (7374355500) and **Ningxi Wang** (3605565772) :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:00:30.210992Z",
     "start_time": "2022-04-06T18:00:28.777704Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.052428Z",
     "start_time": "2022-03-29T07:40:38.051112Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the **Amazon Toy Reviews Dataset (both positive and negative)**, **process the reviews**.\n",
    "This means you should think briefly about:\n",
    "* what stopwords to remove (should you add any custom stopwords to the set? Remove any stopwords?)\n",
    "* what regex cleaning you may need to perform (for example, are there different ways of saying `broken` that you need to account for?)\n",
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Next, **count-vectorize the dataset**. Use the **`sklearn.feature_extraction.text.CountVectorizer`** examples from `Linear Algebra, Distance and Similarity (Completed).ipynb` and `Text Preprocessing Techniques (Completed).ipynb`.\n",
    "\n",
    "I do not want redundant features - for instance, I do not want `Christmas` and `Christ-mas` to be two distinct columns in your document-term matrix. Therefore, I'll be taking a look to make sure you've properly performed your cleaning, stopword removal, etc. to reduce the number of dimensions in your dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.063128Z",
     "start_time": "2022-03-29T07:40:38.053869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'amazon',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'toy',\n",
       " 'toys',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stpw = set(stopwords.words('english'))\n",
    "stpw |= set(['amazon', 'toy', 'toys', 'review', 'reviews'])\n",
    "stpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.066243Z",
     "start_time": "2022-03-29T07:40:38.064610Z"
    }
   },
   "outputs": [],
   "source": [
    "# rev_gd = open('../datasets/good_amazon_toy_reviews.txt', 'r')\n",
    "# rev_pr = open('../datasets/poor_amazon_toy_reviews.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.297674Z",
     "start_time": "2022-03-29T07:40:38.067477Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_gd = pd.read_csv('../datasets/good_amazon_toy_reviews.txt', header=None, encoding='utf8')\n",
    "rev_pr = pd.read_csv('../datasets/poor_amazon_toy_reviews.txt', header=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.310463Z",
     "start_time": "2022-03-29T07:40:38.299372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great quality wooden track (better than some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my daughter loved it and i liked the price and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great item. Pictures pop thru and add detail a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was pleased with the product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114879</th>\n",
       "      <td>It's a piece of junk...doesn't charge multiple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114880</th>\n",
       "      <td>Really small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114881</th>\n",
       "      <td>It is contained in glass which is dangerous if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114882</th>\n",
       "      <td>Fake. Not original. Every time my 5 yr old kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114883</th>\n",
       "      <td>Poor quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114884 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review\n",
       "0                                            Excellent!!!\n",
       "1       Great quality wooden track (better than some o...\n",
       "2       my daughter loved it and i liked the price and...\n",
       "3       Great item. Pictures pop thru and add detail a...\n",
       "4                         I was pleased with the product.\n",
       "...                                                   ...\n",
       "114879  It's a piece of junk...doesn't charge multiple...\n",
       "114880                                       Really small\n",
       "114881  It is contained in glass which is dangerous if...\n",
       "114882  Fake. Not original. Every time my 5 yr old kid...\n",
       "114883                                       Poor quality\n",
       "\n",
       "[114884 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df = pd.concat([rev_gd, rev_pr], axis=0, ignore_index=True)\n",
    "rev_df.columns = ['review']\n",
    "rev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:38.315154Z",
     "start_time": "2022-03-29T07:40:38.312043Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(lines, delimiter=\" \"):\n",
    "    words = Counter() # instantiate a Counter object called words\n",
    "    for line in lines:\n",
    "        for word in line.split(delimiter):\n",
    "            word = word.lower()\n",
    "            if word in stpw: continue\n",
    "            words[word] += 1 # increment count for word\n",
    "    return words\n",
    "\n",
    "# def count_words(doc):\n",
    "#     counts = Counter()\n",
    "#     for r in doc:\n",
    "#         counts_tmp = Counter(re.findall(r'\\w\\w+', r, flags=re.IGNORECASE))\n",
    "#         counts += counts_tmp\n",
    "#     return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.290846Z",
     "start_time": "2022-03-29T07:40:38.317810Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 52277),\n",
       " ('great', 24128),\n",
       " ('love', 16701),\n",
       " ('loves', 13551),\n",
       " ('one', 12102),\n",
       " ('it.', 10472),\n",
       " ('like', 10194),\n",
       " ('good', 9927),\n",
       " ('little', 9809),\n",
       " ('old', 9549),\n",
       " ('/><br', 8375),\n",
       " ('loved', 8279),\n",
       " ('would', 8167),\n",
       " ('year', 8158),\n",
       " ('fun', 8157),\n",
       " ('really', 8065),\n",
       " ('kids', 7720),\n",
       " ('get', 7499),\n",
       " ('bought', 7008),\n",
       " ('well', 6862),\n",
       " ('son', 6425),\n",
       " ('perfect', 6320),\n",
       " ('game', 6295),\n",
       " ('daughter', 6204),\n",
       " ('got', 6093),\n",
       " ('play', 6088),\n",
       " ('nice', 5397),\n",
       " ('product', 5375),\n",
       " ('easy', 5341),\n",
       " ('even', 5266),\n",
       " ('much', 5108),\n",
       " ('quality', 5029),\n",
       " ('time', 4921),\n",
       " ('made', 4673),\n",
       " ('it!', 4629),\n",
       " ('use', 4558),\n",
       " ('also', 4549),\n",
       " ('set', 4430),\n",
       " ('put', 4211),\n",
       " ('cute', 4105),\n",
       " ('buy', 4050),\n",
       " ('gift', 3985),\n",
       " ('2', 3984),\n",
       " ('make', 3745),\n",
       " ('still', 3627),\n",
       " ('came', 3610),\n",
       " ('-', 3608),\n",
       " ('first', 3595),\n",
       " ('two', 3471),\n",
       " ('recommend', 3424),\n",
       " ('grandson', 3405),\n",
       " ('3', 3397),\n",
       " ('playing', 3296),\n",
       " ('price', 3190),\n",
       " ('received', 3153),\n",
       " ('looks', 3146),\n",
       " (\"i'm\", 3142),\n",
       " ('small', 3141),\n",
       " ('used', 3079),\n",
       " ('happy', 3019),\n",
       " ('birthday', 3005),\n",
       " ('super', 2847),\n",
       " ('size', 2818),\n",
       " ('back', 2785),\n",
       " ('could', 2785),\n",
       " ('big', 2777),\n",
       " ('best', 2763),\n",
       " ('them.', 2760),\n",
       " ('awesome', 2741),\n",
       " ('lot', 2714),\n",
       " ('excellent', 2672),\n",
       " ('go', 2633),\n",
       " ('arrived', 2598),\n",
       " ('way', 2584),\n",
       " ('box', 2576),\n",
       " ('new', 2524),\n",
       " ('pretty', 2501),\n",
       " ('fast', 2500),\n",
       " ('work', 2497),\n",
       " ('pieces', 2491),\n",
       " ('around', 2491),\n",
       " ('many', 2459),\n",
       " ('different', 2438),\n",
       " ('works', 2435),\n",
       " ('5', 2430),\n",
       " ('look', 2423),\n",
       " ('worth', 2420),\n",
       " ('item', 2411),\n",
       " ('fit', 2408),\n",
       " ('4', 2385),\n",
       " ('enough', 2367),\n",
       " ('see', 2341),\n",
       " ('definitely', 2339),\n",
       " ('granddaughter', 2305),\n",
       " ('think', 2291),\n",
       " ('purchased', 2279),\n",
       " ('every', 2269),\n",
       " ('better', 2263),\n",
       " ('it,', 2257),\n",
       " ('take', 2255),\n",
       " ('exactly', 2250),\n",
       " ('cards', 2235),\n",
       " ('comes', 2229),\n",
       " ('thing', 2203),\n",
       " ('absolutely', 2195),\n",
       " (\"can't\", 2188),\n",
       " ('right', 2157),\n",
       " ('keep', 2128),\n",
       " ('looking', 2123),\n",
       " ('makes', 2121),\n",
       " ('&', 2098),\n",
       " ('doll', 2023),\n",
       " ('going', 2001),\n",
       " ('since', 1992),\n",
       " ('want', 1989),\n",
       " ('need', 1973),\n",
       " ('thank', 1971),\n",
       " ('plastic', 1966),\n",
       " ('come', 1961),\n",
       " ('together', 1947),\n",
       " ('figure', 1944),\n",
       " ('long', 1885),\n",
       " ('another', 1865),\n",
       " ('find', 1861),\n",
       " ('well.', 1837),\n",
       " ('never', 1822),\n",
       " ('product.', 1815),\n",
       " ('bit', 1802),\n",
       " ('sure', 1802),\n",
       " ('/>i', 1778),\n",
       " ('know', 1777),\n",
       " ('highly', 1775),\n",
       " ('child', 1764),\n",
       " ('time.', 1761),\n",
       " (\"i've\", 1750),\n",
       " ('game.', 1734),\n",
       " ('water', 1720),\n",
       " ('able', 1700),\n",
       " ('hard', 1695),\n",
       " ('ordered', 1692),\n",
       " ('favorite', 1691),\n",
       " ('using', 1655),\n",
       " ('colors', 1653),\n",
       " ('years', 1653),\n",
       " ('thought', 1651),\n",
       " ('party', 1613),\n",
       " ('everything', 1613),\n",
       " ('actually', 1610),\n",
       " ('played', 1600),\n",
       " ('card', 1589)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = count_words(rev_df['review'])\n",
    "counts.most_common(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> by comparing common words with default stopwords, add more stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.294602Z",
     "start_time": "2022-03-29T07:40:39.292736Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['would', 'really', 'get', 'got', 'even', 'much', 'also', 'item', 'every', \n",
    "            'definitely', 'exactly', 'absolutely', 'actually', 'able'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> by analyzing previous conunt-vectorize results, add more stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.298131Z",
     "start_time": "2022-03-29T07:40:39.296071Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['almost', 'always', 'another', 'could', 'something', 'thing', 'must', 'never',\n",
    "            'us', 'me',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regex cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.304433Z",
     "start_time": "2022-03-29T07:40:39.299640Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_df['rev_std'] = rev_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.307908Z",
     "start_time": "2022-03-29T07:40:39.305613Z"
    }
   },
   "outputs": [],
   "source": [
    "### too slow\n",
    "# def standardize_word(doc, word_orig, word_std):\n",
    "#     for i in tqdm(range(len(doc))):\n",
    "#         doc.loc[i, 'rev_std'] = re.sub(word_orig, word_std, doc.loc[i, 'review'], flags=re.IGNORECASE)\n",
    "\n",
    "def standardize_word(doc, word_orig, word_std):\n",
    "    doc = doc.str.replace(word_orig, word_std,\n",
    "                          flags=re.IGNORECASE, regex=True)\n",
    "    return doc # has to return, otherwise local var wont affect global var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we noticed considerable number of `&#34;xxx&#34;, <br />` exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.374318Z",
     "start_time": "2022-03-29T07:40:39.308896Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'(<br />)', ''\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.378921Z",
     "start_time": "2022-03-29T07:40:39.375453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I got this item for me and my son to play around with. The closest relevance I have to items like these is while in the army I was trained in the camera rc bots. This thing is awesome we tested the range and got somewhere close to 50 yards without an issue. Getting the controls is a bit tricky at first but after about twenty minutes you get the feel for it. The drone comes just about fly ready you just have to sync the controller. I am definitely a fan of the drones now. Only concern I have is maybe a little more silent but other than that great buy.<br /><br />*Disclaimer I received this product at a discount for my unbiased review.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df.iloc[10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:39.382804Z",
     "start_time": "2022-03-29T07:40:39.380137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I got this item for me and my son to play around with. The closest relevance I have to items like these is while in the army I was trained in the camera rc bots. This thing is awesome we tested the range and got somewhere close to 50 yards without an issue. Getting the controls is a bit tricky at first but after about twenty minutes you get the feel for it. The drone comes just about fly ready you just have to sync the controller. I am definitely a fan of the drones now. Only concern I have is maybe a little more silent but other than that great buy.*Disclaimer I received this product at a discount for my unbiased review.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:41.773469Z",
     "start_time": "2022-03-29T07:40:39.383948Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'(&#[0-9]+;|)', '' # no \\b\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:41.797943Z",
     "start_time": "2022-03-29T07:40:41.774664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rev_df['rev_std'] != rev_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:41.801705Z",
     "start_time": "2022-03-29T07:40:41.799211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great item. Pictures pop thru and add detail as &#34;painted.&#34;  Pictures dry and it can be repainted.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df.iloc[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:41.805450Z",
     "start_time": "2022-03-29T07:40:41.802924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great item. Pictures pop thru and add detail as painted.  Pictures dry and it can be repainted.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_df.iloc[3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:41.808215Z",
     "start_time": "2022-03-29T07:40:41.806640Z"
    }
   },
   "outputs": [],
   "source": [
    "### it invalids stopwords, changes didn't -> didnt\n",
    "# word_orig, word_std = '(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?', ''\n",
    "# rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> as stemming treats word with punctuations differently from the word itself, remove punctuations first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:42.036869Z",
     "start_time": "2022-03-29T07:40:41.809540Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'[.!?\\-\\\"\\\\]', ' '\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> also, we notice some synonyms and standardize them before count-vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:43.161152Z",
     "start_time": "2022-03-29T07:40:42.038043Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'\\b((christ|x)(?:-)?mas)\\b', 'christmas'\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)\n",
    "word_orig, word_std = r'\\bb(?:irth)?(?:-)?day(?:s)?\\b', 'birthday'\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)\n",
    "word_orig, word_std = r'\\b(y(?:ea)?r(?:s)?)\\b', 'year'\n",
    "rev_df['rev_std'] = standardize_word(rev_df['rev_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:40:43.170326Z",
     "start_time": "2022-03-29T07:40:43.165727Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    ref:\n",
    "https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "'''\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:47:37.490384Z",
     "start_time": "2022-03-29T07:40:43.174935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114884/114884 [06:54<00:00, 277.30it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in tqdm(range(len(rev_df))):\n",
    "    rev_df.loc[i, 'rev_std'] = lemmatize_sentence(rev_df.loc[i, 'rev_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Reason for choosing lemmatization over stemming:**\n",
    "when lacking of contexts, stemming may discriminate between words with different meanings. However, lemmatization is a way to reduce word-form according to context. So, we prefer lemmatization in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count-vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:47:39.246081Z",
     "start_time": "2022-03-29T07:47:37.492648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (114884, 90)\n",
      "Total number of occurences: 484908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>around</th>\n",
       "      <th>awesome</th>\n",
       "      <th>back</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>birthday</th>\n",
       "      <th>box</th>\n",
       "      <th>buy</th>\n",
       "      <th>card</th>\n",
       "      <th>child</th>\n",
       "      <th>...</th>\n",
       "      <th>together</th>\n",
       "      <th>try</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   around  awesome  back  best  big  birthday  box  buy  card  child  ...  \\\n",
       "0       0        0     0     0    0         0    0    0     0      0  ...   \n",
       "1       0        0     0     0    0         0    0    0     0      0  ...   \n",
       "2       1        0     0     1    0         0    0    0     0      0  ...   \n",
       "3       0        0     0     0    0         0    0    0     0      0  ...   \n",
       "4       0        0     0     0    0         0    0    0     0      0  ...   \n",
       "\n",
       "   together  try  two  use  want  way  well  work  worth  year  \n",
       "0         0    0    0    0     0    0     0     0      0     0  \n",
       "1         0    1    0    0     0    0     0     0      0     0  \n",
       "2         0    0    0    0     0    1     0     0      0     0  \n",
       "3         0    0    0    0     0    0     0     0      0     0  \n",
       "4         0    0    0    0     0    0     0     0      0     0  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stpw, binary=True, min_df=0.02)\n",
    "X = vectorizer.fit_transform(rev_df['rev_std'])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:47:39.251284Z",
     "start_time": "2022-03-29T07:47:39.247276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "around \tawesome \tback \tbest \tbig \tbirthday \tbox \tbuy \tcard \tchild \tcolor \tcome \tcute \tdaughter \tday \tdoll \teasy \tenjoy \tenough \texcellent \texpect \tfast \tfigure \tfind \tfirst \tfit \tfun \tgame \tgift \tgive \tgo \tgood \tgranddaughter \tgrandson \tgreat \thappy \thold \tkeep \tkid \tknow \tlike \tlittle \tlong \tlook \tlot \tlove \tmake \tmoney \tmonth \tneed \tnew \tnice \told \tone \torder \tparty \tperfect \tpicture \tpiece \tplay \tpretty \tprice \tproduct \tpurchase \tput \tquality \treceive \trecommend \tright \tsay \tsee \tset \tsize \tsmall \tson \tstill \tsuper \ttake \tthink \ttime \ttogether \ttry \ttwo \tuse \twant \tway \twell \twork \tworth \tyear \t"
     ]
    }
   ],
   "source": [
    "for i in vectorized_df.columns:\n",
    "    print(i, '\\t', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. **Stopwords, Stemming, Lemmatization Practice**\n",
    "\n",
    "Using the **McDonalds Negative Reviews** file from Week 1:\n",
    "* Count-vectorize the corpus. Treat each sentence as a document.\n",
    "\n",
    "How many features (dimensions) do you get when you:\n",
    "* Perform **stemming** and then count-vectorization\n",
    "* Perform **lemmatization** and then **count-vectorization**.\n",
    "* Perform **lemmatization**, remove **stopwords**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:00:39.891340Z",
     "start_time": "2022-04-06T18:00:39.846495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>679500008</td>\n",
       "      <td>Portland</td>\n",
       "      <td>I enjoyed the part where I repeatedly asked if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>679500224</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Worst McDonalds I've been in in a long time! D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>679500608</td>\n",
       "      <td>New York</td>\n",
       "      <td>When I am really craving for McDonald's, this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>679501257</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Two points right out of the gate: 1. Thuggery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>679501402</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>I wanted to grab breakfast one morning before ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id         city  \\\n",
       "0     679455653      Atlanta   \n",
       "1     679455654      Atlanta   \n",
       "2     679455655      Atlanta   \n",
       "3     679455656      Atlanta   \n",
       "4     679455657      Atlanta   \n",
       "...         ...          ...   \n",
       "1520  679500008     Portland   \n",
       "1521  679500224      Houston   \n",
       "1522  679500608     New York   \n",
       "1523  679501257      Chicago   \n",
       "1524  679501402  Los Angeles   \n",
       "\n",
       "                                                 review  \n",
       "0     I'm not a huge mcds lover, but I've been to be...  \n",
       "1     Terrible customer service. I came in at 9:30pm...  \n",
       "2     First they \"lost\" my order, actually they gave...  \n",
       "3     I see I'm not the only one giving 1 star. Only...  \n",
       "4     Well, it's McDonald's, so you know what the fo...  \n",
       "...                                                 ...  \n",
       "1520  I enjoyed the part where I repeatedly asked if...  \n",
       "1521  Worst McDonalds I've been in in a long time! D...  \n",
       "1522  When I am really craving for McDonald's, this ...  \n",
       "1523  Two points right out of the gate: 1. Thuggery ...  \n",
       "1524  I wanted to grab breakfast one morning before ...  \n",
       "\n",
       "[1525 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcd = pd.read_csv('../datasets/mcdonalds-yelp-negative-reviews.csv', encoding='latin1')\n",
    "mcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:12.129796Z",
     "start_time": "2022-03-29T07:53:12.112036Z"
    }
   },
   "outputs": [],
   "source": [
    "mcd['rev_stem'] = np.nan\n",
    "mcd['rev_lem'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:01:01.372528Z",
     "start_time": "2022-04-06T18:01:01.366089Z"
    }
   },
   "outputs": [],
   "source": [
    "mcd['rev_lem_woPOS'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:13.283973Z",
     "start_time": "2022-03-29T07:53:13.281489Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemming_sentence(sentence):\n",
    "    stemmed_sentence = []\n",
    "    for word in sentence.split(' '):      \n",
    "        stemmed_sentence.append(stemmer.stem(word))\n",
    "    return \" \".join(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:16.539735Z",
     "start_time": "2022-03-29T07:53:14.296639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:02<00:00, 681.22it/s]\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "for i in tqdm(range(len(mcd))):\n",
    "    mcd.loc[i, 'rev_stem'] = stemming_sentence(mcd.loc[i, 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:17.679963Z",
     "start_time": "2022-03-29T07:53:17.561009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7638"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mcd['rev_stem'])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "dim_stem = vectorized_df.shape[1]\n",
    "dim_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:25.532701Z",
     "start_time": "2022-03-29T07:53:19.421628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:06<00:00, 249.68it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in tqdm(range(len(mcd))):\n",
    "    mcd.loc[i, 'rev_lem'] = lemmatize_sentence(mcd.loc[i, 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:27.118528Z",
     "start_time": "2022-03-29T07:53:26.996218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7191"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mcd['rev_lem'])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "dim_lem = vectorized_df.shape[1]\n",
    "dim_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:28.771991Z",
     "start_time": "2022-03-29T07:53:28.648343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6910"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(mcd['rev_lem'])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "dim_lem_stp = vectorized_df.shape[1]\n",
    "dim_lem_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization w/o POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:03:37.460601Z",
     "start_time": "2022-04-06T18:03:37.455073Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence_woPOS(sentence):\n",
    "    lemmatized_sentence = []\n",
    "    for word in sentence.split(' '):      \n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:03:39.122970Z",
     "start_time": "2022-04-06T18:03:38.406224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:00<00:00, 2159.35it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in tqdm(range(len(mcd))):\n",
    "    mcd.loc[i, 'rev_lem_woPOS'] = lemmatize_sentence_woPOS(mcd.loc[i, 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T18:08:02.963911Z",
     "start_time": "2022-04-06T18:08:02.797059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mcd['rev_lem_woPOS'])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "dim_lem_woPOS = vectorized_df.shape[1]\n",
    "dim_lem_woPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:53:30.380439Z",
     "start_time": "2022-03-29T07:53:30.378143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features (dimensions) after\n",
      "\tstemming & count-vectorization: 7638.\n",
      "\tlemmatization & count-vectorization: 7191.\n",
      "\tlemmatization, removing stopwords & count-vectorization: 6910.\n"
     ]
    }
   ],
   "source": [
    "print('# of features (dimensions) after')\n",
    "print(f'\\tstemming & count-vectorization: {dim_stem}.')\n",
    "print(f'\\tlemmatization & count-vectorization: {dim_lem}.')\n",
    "print(f'\\tlemmatization, removing stopwords & count-vectorization: {dim_lem_stp}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
