{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word Count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Counter\" data-toc-modified-id=\"Using-Counter-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Using <code>Counter</code></a></span></li><li><span><a href=\"#Adding-Word-Counts-From-Two-Distinct-Datasets-Together\" data-toc-modified-id=\"Adding-Word-Counts-From-Two-Distinct-Datasets-Together-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Adding Word Counts From Two Distinct Datasets Together</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords-Using-gensim\" data-toc-modified-id=\"Removing-Stopwords-Using-gensim-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Removing Stopwords Using <code>gensim</code></a></span></li><li><span><a href=\"#Finding-Similar-Word-Matches-Using-difflib\" data-toc-modified-id=\"Finding-Similar-Word-Matches-Using-difflib-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Finding Similar Word Matches Using <code>difflib</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Fuzzy-Matching\" data-toc-modified-id=\"Fuzzy-Matching-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Fuzzy Matching</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Cases\" data-toc-modified-id=\"Use-Cases-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Use Cases</a></span></li><li><span><a href=\"#Limitations\" data-toc-modified-id=\"Limitations-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Limitations</a></span></li><li><span><a href=\"#Slow-Performance\" data-toc-modified-id=\"Slow-Performance-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Slow Performance</a></span></li><li><span><a href=\"#Not-&quot;Language-Aware&quot;\" data-toc-modified-id=\"Not-&quot;Language-Aware&quot;-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Not \"Language Aware\"</a></span></li></ul></li><li><span><a href=\"#Install-the-Dependencies-if-Necessary\" data-toc-modified-id=\"Install-the-Dependencies-if-Necessary-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Install the Dependencies if Necessary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Token-Set-Ratio-(following-examples-from-fuzzywuzzy's-documentation)\" data-toc-modified-id=\"Token-Set-Ratio-(following-examples-from-fuzzywuzzy's-documentation)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Token Set Ratio (following examples from <code>fuzzywuzzy</code>'s documentation)</a></span></li><li><span><a href=\"#Token-Set-Ratio\" data-toc-modified-id=\"Token-Set-Ratio-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Token Set Ratio</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "## Using `Counter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal dictionary object will return a key error if you do not first initialize the key value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:40:22.644556Z",
     "start_time": "2022-03-30T01:40:22.546443Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'yu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b4cffd8d4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'yu'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "ordinary_dict: Dict = dict()\n",
    "ordinary_dict[\"yu\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object in `collections` has a default value of 0 for every key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:40:23.759304Z",
     "start_time": "2022-03-30T01:40:23.743246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yu': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "counter[\"yu\"] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the you can pass in a list of strings to the `Counter` constructor, as well as calling the\n",
    "`most_common` method to get the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:40:34.059212Z",
     "start_time": "2022-03-30T01:40:34.007099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7363), ('and', 4727), ('of', 3944), ('to', 3398), ('a', 2792)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "words: List[str] = open(\"../datasets/tale-of-two-cities.txt\").read().split()\n",
    "dickens_counter = Counter(words)\n",
    "dickens_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also quickly use this counter to find the percentage of words in a corpus that belong to a certain word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:42:50.415872Z",
     "start_time": "2022-03-30T01:42:50.396218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054036400998091885"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dickens_counter[\"the\"] / sum(dickens_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Word Counts From Two Distinct Datasets Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add two `Counter` objects together to get their combined counts. In this example, we'll load in the `fraudulent_emails.txt` dataset and start a new counter called `email_counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:42:55.255164Z",
     "start_time": "2022-03-30T01:42:55.240291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_counter = Counter(open(\"../datasets/fraudulent_emails.txt\").read().split())\n",
    "email_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:42:56.582383Z",
     "start_time": "2022-03-30T01:42:56.553132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7504), ('and', 4789), ('of', 4024), ('to', 3514), ('a', 2834)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_counter: Counter = dickens_counter + email_counter\n",
    "combined_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also subtract counts from one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:43:09.953816Z",
     "start_time": "2022-03-30T01:43:09.915713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get back the original email_counter\n",
    "(combined_counter - dickens_counter).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords Using `gensim`\n",
    "\n",
    "Removing stopwords in `nltk` often means you first have to tokenize the document into distinct tokens, then run each token through to check if it is a stopword. Another commonly used NLP library in Python, `gensim`, has a helper function to do this all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rendered manner desperate, state beckoning conductor, drew neck arm shook shoulder, lifted little, hurried room. He sat door, held her, clinging him.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = '''\n",
    "Rendered in a manner desperate, by her state and by the beckoning of their conductor,\n",
    "he drew over his neck the arm that shook upon his shoulder, lifted her a little, and hurried \n",
    "her into the room. He sat her down just within the door, and held her, clinging to him.\n",
    "'''\n",
    "processed_text = remove_stopwords(text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "it keeps punctuations rather than replace them with white spaces</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, this only works well if you are happy with Gensim's only predefined list of stopwords. To inspect what stopwords are used in Gensim, use\n",
    "```python\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "print(STOPWORDS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:51:03.774154Z",
     "start_time": "2022-03-30T01:50:59.181748Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'cant', 'few', 'his', 'during', 'whole', 'beside', 'per', 'it', 'mine', 'now', 'whatever', 'latter', 'to', 'that', 'four', 'find', 'more', 'something', 'among', 'i', 'must', 'him', 'next', 'down', 'most', 'etc', 'alone', 'afterwards', 'side', 'nobody', 'last', 'whereby', 'therein', 're', 'thru', 'never', 'nowhere', 'how', 'above', 'rather', 'name', 'them', 'those', 'fill', 'kg', 'does', 'between', 'towards', 'six', 'along', 'someone', 'con', 'from', 'become', 'your', 'used', 'perhaps', 'though', 'without', 'former', 'although', 'her', 'everything', 'for', 'these', 'at', 'front', 'did', 'describe', 'seems', 'seem', 'also', 'move', 'keep', 'further', 'see', 'thereafter', 'take', 'done', 'full', 'bill', 'anyhow', 'amongst', 'upon', 'bottom', 'should', 'fifteen', 'therefore', 'serious', 'such', 'call', 'every', 'five', 'empty', 'being', 'two', 'first', 'somewhere', 'besides', 'everyone', 'do', 'doing', 'us', 'km', 'made', 'cry', 'amoungst', 'into', 'either', 'hereupon', 'am', 'as', 'however', 'well', 'herein', 'nothing', 'below', 'what', 'which', 'already', 'quite', 'beyond', 'none', 'forty', 'de', 'thin', 'yourself', 'get', 'hundred', 'and', 'not', 'off', 'some', 'thus', 'sometimes', 'inc', 'with', 'became', 'the', 'others', 'whenever', 'out', 'ltd', 'wherever', 'go', 'somehow', 'by', 'we', 'be', 'here', 'whereas', 'cannot', 'onto', 'whereupon', 'everywhere', 'all', 'in', 'beforehand', 'could', 'found', 'while', 'or', 'if', 'noone', 'been', 'may', 'detail', 'of', 'were', 'otherwise', 'give', 'much', 'whereafter', 'whither', 'can', 'any', 'under', 'un', 'you', 'yours', 'computer', 'about', 'thereupon', 'another', 'except', 'several', 'yourselves', 'mill', 'its', 'whether', 'via', 'becomes', 'herself', 'mostly', 'indeed', 'one', 'enough', 'than', 'using', 'through', 'twelve', 'where', 'nevertheless', 'ten', 'due', 'once', 'himself', 'namely', 'too', 'together', 'top', 'a', 'are', 'often', 'he', 'seemed', 'anywhere', 'own', 'so', 'least', 'put', 'my', 'on', 'didn', 'will', 'ie', 'she', 'throughout', 'very', 'really', 'but', 'anything', 'behind', 'formerly', 'after', 'had', 'both', 'eight', 'whoever', 'toward', 'thick', 'sincere', 'always', 'three', 'no', 'hereafter', 'ourselves', 'just', 'eleven', 'have', 'fifty', 'against', 'else', 'elsewhere', 'show', 'their', 'anyone', 'across', 'less', 'an', 'various', 'latterly', 'over', 'up', 'unless', 'thence', 'this', 'hasnt', 'whom', 'neither', 'hers', 'then', 'whose', 'itself', 'there', 'interest', 'yet', 'thereby', 'each', 'twenty', 'within', 'even', 'our', 'system', 'co', 'wherein', 'hereby', 'make', 'who', 'when', 'myself', 'still', 'many', 'themselves', 'back', 'they', 'ours', 'might', 'me', 'is', 'meanwhile', 'nor', 'say', 'seeming', 'anyway', 'has', 'why', 'sometime', 'was', 'ever', 'third', 'becoming', 'part', 'whence', 'hence', 'until', 'would', 'because', 'around', 'amount', 'couldnt', 'again', 'before', 'sixty', 'please', 'doesn', 'only', 'regarding', 'eg', 'don', 'almost', 'moreover', 'fire', 'same', 'since', 'other', 'nine'})\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "hard to modify as it's a frozon set.\n",
    "<b><i>treatment</i></b>: remove domain knowledge stopwords after count vectorization by deleting cols</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Word Matches Using `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "    <p><b>edit distance</b> dog -> dag, edit distance=1(most algorithms)\n",
    "    <p>most algorithms are aimed for shortest edit distance\n",
    "    <p>brute force, i.e. O(N!) is terrible\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Python's Standard Library, the `difflib` has a variety of tools for helping identify differences between text and content. It uses an algorithm called the **Ratcliff-Obershelp algorithm**, which is described in brief below:\n",
    "\n",
    "> The idea is to find the longest contiguous matching subsequence that contains no “junk” elements; these “junk” elements are ones that are uninteresting in some sense, such as blank lines or whitespace. (Handling junk is an extension to the Ratcliff and Obershelp algorithm.) The same idea is then applied recursively to the pieces of the sequences to the left and to the right of the matching subsequence. This does not yield minimal edit sequences, but does tend to yield matches that “look right” to people. [Link](https://docs.python.org/3/library/difflib.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:56:24.457588Z",
     "start_time": "2022-03-30T01:56:24.413406Z"
    }
   },
   "outputs": [],
   "source": [
    "# this loads in the top 20k most popular words in the English language\n",
    "words = set(map(lambda word: word.replace(\"\\n\", \"\"), open(\"../datasets/20k.txt\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:56:31.225825Z",
     "start_time": "2022-03-30T01:56:31.152504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight', 'naughty', 'knights']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "w = \"knaght\"\n",
    "difflib.get_close_matches(w, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine this with a tokenizer to create your own (very basic) spellcheck function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is a crazy person'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def spellcheck_document(text):\n",
    "    new_tokens = []\n",
    "    for token in word_tokenize(text):\n",
    "        matches = difflib.get_close_matches(token.lower(), words, n=1, cutoff=0.7)\n",
    "        if len(matches) == 0 or token.lower() in words:\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(matches[0])\n",
    "    return \" \".join(new_tokens)\n",
    "spellcheck_document(\"He is a craezy perzon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "it will encounter name identity issues, however. and decimal issues\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "cutoff should be adjusted given the length of words. write some customized smoothing func\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching refers to \"approximate matching\", where we are allowed a certain degree of error between the query value and the search result. \n",
    "\n",
    "The `fuzzywuzzy` library uses a distance measure called **Levenshtein Distance** which describes the minimum number of operations to transform one string into another.\n",
    "\n",
    "* `cat` $\\rightarrow$ `cat` : `0` distance\n",
    "* `dog` $\\rightarrow$ `door`: `2` distance\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "* spell checking\n",
    "* DNA analysis\n",
    "* authorship/plagiarism detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "Levenshtein Distance: O(N*M), considerably slow yet much better compared to brute force, where you have to check every single sequence of a word</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T02:12:40.089381Z",
     "start_time": "2022-03-30T02:12:40.052246Z"
    }
   },
   "outputs": [],
   "source": [
    "# this loads in the top 20k most popular words in the English language\n",
    "words = set(map(lambda word: word.replace(\"\\n\", \"\"), open(\"../datasets/20k.txt\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T02:14:50.823172Z",
     "start_time": "2022-03-30T02:14:50.566687Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T02:14:52.832173Z",
     "start_time": "2022-03-30T02:14:52.160686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6583336250000684\n",
      "Best results: [('ruled', 67), ('fulfilled', 63), ('stuffed', 59), ('shuffle', 59), ('perfume', 59)]\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "target = \"kerfuffled\"\n",
    "\n",
    "\n",
    "start = timer()\n",
    "for _ in range(10):\n",
    "    bests = process.extractBests(target, words, scorer=fuzz.ratio)\n",
    "end = timer()\n",
    "print(end - start) # Time in seconds to check 10 words, i.e. 10 targets\n",
    "print(f'Best results: {bests}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "slow performance due to O(N^2), thus use on some domain-based targets only</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to improve efficiency\n",
    "```python\n",
    "cache = {}\n",
    "for _ in range(10):\n",
    "    if (target, word) in cache:\n",
    "        return cache[(target, word)]\n",
    "    bests = process.extractBests(target, words, scorer=fuzz.ratio)\n",
    "    cache[(target, word)] = bests[0]\n",
    "    return cache[(target, word)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T02:14:58.924482Z",
     "start_time": "2022-03-30T02:14:58.905911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"kerfuffled\", \"ruled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not \"Language Aware\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing the classification proposed by the Levenshtein distance to that of the comparative method shows that the Levenshtein classification is correct only 40% of time. Standardizing the orthography increases the performance, but only to a maximum of 65% accuracy within language subgroups. The accuracy of the Levenshtein classification **decreases rapidly with phylogenetic distance**, failing to discriminate homology and chance similarity across distantly related languages.This poor performance suggests the need for more linguistically nuanced methods for automated language classification tasks.\n",
    "\n",
    "[\"Levenshtein distances fail to identify language relationships accurately\" by Simon Greenhill](https://dl.acm.org/doi/10.1162/COLI_a_00073)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Dependencies if Necessary\n",
    "```python\n",
    "!pip3 install fuzzywuzzy\n",
    "!pip3 install python-Levenshtein\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"cat\", \"saturday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"hog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"smithy\", \"smithfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is it symmetric? -Yes\n",
    "fuzz.ratio(\"smithfield\", \"smithy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"photosynthesis\", \"photosynthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does case matter? -Yes\n",
    "fuzz.ratio(\"Photosynthesis\", \"photosynthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what happens if you arbitrarily increase the size of the strings?\n",
    "\n",
    "fuzz.ratio(\"dog\" * 3, \"hog\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Set Ratio (following examples from `fuzzywuzzy`'s documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))\n",
    "print(fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "<p>token_sort_ratio: orders are considered\n",
    "<p>token_set_ratio: for product tags\n",
    "<p>... other ratios\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Set Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\"))\n",
    "print(fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "126.52px",
    "left": "1460.98px",
    "top": "197.952px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
