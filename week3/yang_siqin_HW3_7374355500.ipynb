{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This homework is co-completed by **Siqin Yang** (7374355500) and **Ningxi Wang** (3605565772) :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.172142Z",
     "start_time": "2022-04-12T22:35:38.926517Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/antheayang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textacy.preprocessing.replace import urls, numbers, emojis, currency_symbols\n",
    "from textacy.preprocessing.remove import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, pick 2-4 products, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together? (skip)\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review (i.e. 4/5)\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.229063Z",
     "start_time": "2022-04-12T22:35:42.173405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20986</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>AVTY5M74VA1BJ</td>\n",
       "      <td>tarotqueen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316822400</td>\n",
       "      <td>dogs love greenies</td>\n",
       "      <td>What can I say, dogs love greenies. They begg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20987</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A13TNN54ZEAUB1</td>\n",
       "      <td>dcz2221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316736000</td>\n",
       "      <td>Greenies review</td>\n",
       "      <td>This review is for a box of Greenies Lite for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "3  20986  B002QWP89S   AVTY5M74VA1BJ              tarotqueen   \n",
       "4  20987  B002QWP89S  A13TNN54ZEAUB1                 dcz2221   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "3                     1                       1      5  1316822400   \n",
       "4                     1                       1      5  1316736000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "3                               dogs love greenies   \n",
       "4                                  Greenies review   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  \n",
       "3  What can I say, dogs love greenies. They begg ...  \n",
       "4  This review is for a box of Greenies Lite for ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz = pd.read_csv('../datasets/amazon_fine_foods.csv')\n",
    "amz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete duplicated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.244704Z",
     "start_time": "2022-04-12T22:35:42.230699Z"
    }
   },
   "outputs": [],
   "source": [
    "amz.drop_duplicates('Text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.248961Z",
     "start_time": "2022-04-12T22:35:42.246250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4931, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, this does not drop the following duplicates as they have difference in space (single space vs double space)\n",
    "\n",
    "Our dogs love Greenies, but of course, which doggies don't?  I bought this for my dashchund and minpin, and it's perfect!  A great price for a great product.  Who could ask for more.\n",
    "\n",
    "----\n",
    "Our dogs love Greenies, but of course, which doggies don't? I bought this for my dashchund and minpin, and it's perfect! A great price for a great product. Who could ask for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.253796Z",
     "start_time": "2022-04-12T22:35:42.250003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      Our dogs love Greenies, but of course, which d...\n",
       "317    Our dogs love Greenies, but of course, which d...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.loc[amz['Id'].isin([20985,21300]), 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.258354Z",
     "start_time": "2022-04-12T22:35:42.254849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.loc[amz['Id']==20985, 'Text'].values == amz.loc[amz['Id']==21300, 'Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.269749Z",
     "start_time": "2022-04-12T22:35:42.259395Z"
    }
   },
   "outputs": [],
   "source": [
    "amz['Text'] = amz['Text'].str.replace(\"  \", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.281280Z",
     "start_time": "2022-04-12T22:35:42.272123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4930, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.drop_duplicates('Text', inplace=True)\n",
    "amz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick certain products for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.289001Z",
     "start_time": "2022-04-12T22:35:42.283058Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B007JFMH8M', 'B002QWP89S', 'B003B3OOPA', 'B001EO5Q64', 'B0013NUGDE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = amz.groupby('ProductId').count()['Id'].sort_values(ascending=False).head(5).index\n",
    "products = list(products)\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.296650Z",
     "start_time": "2022-04-12T22:35:42.290078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>productid</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>5</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>5</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>5</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20986</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>5</td>\n",
       "      <td>What can I say, dogs love greenies. They begg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20987</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>5</td>\n",
       "      <td>This review is for a box of Greenies Lite for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   productid  score                                               text\n",
       "0  20983  B002QWP89S      5  my 12 year old sheltie has chronic brochotitis...\n",
       "1  20984  B002QWP89S      5  These are genuine Greenies product, not a knoc...\n",
       "2  20985  B002QWP89S      5  Our dogs love Greenies, but of course, which d...\n",
       "3  20986  B002QWP89S      5  What can I say, dogs love greenies. They begg ...\n",
       "4  20987  B002QWP89S      5  This review is for a box of Greenies Lite for ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_prd = amz.loc[amz['ProductId'].isin(products), ['Id', 'ProductId', 'Score', 'Text']]\n",
    "amz_prd.columns = ['id', 'productid', 'score', 'text']\n",
    "amz_prd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: why remove/keep stopwords?\n",
    "- A: stopwords are useless in analyzing as they don't convey any information in patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.300646Z",
     "start_time": "2022-04-12T22:35:42.297627Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.303324Z",
     "start_time": "2022-04-12T22:35:42.301619Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['amz', 'amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.308552Z",
     "start_time": "2022-04-12T22:35:42.304530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'amazon',\n",
       " 'amz',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.312121Z",
     "start_time": "2022-04-12T22:35:42.309725Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(lines, delimiter=\" \"):\n",
    "    words = Counter() # instantiate a Counter object called words\n",
    "    for line in lines:\n",
    "        for word in line.split(delimiter):\n",
    "            word = word.lower()\n",
    "            if word in stpw: continue\n",
    "            words[word] += 1 # increment count for word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.390126Z",
     "start_time": "2022-04-12T22:35:42.313266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coconut', 1558),\n",
       " ('oil', 1384),\n",
       " ('like', 1184),\n",
       " ('love', 1098),\n",
       " ('/><br', 1024),\n",
       " ('great', 1017),\n",
       " ('use', 965),\n",
       " ('good', 827),\n",
       " ('one', 814),\n",
       " ('product', 697),\n",
       " ('taste', 689),\n",
       " ('soft', 644),\n",
       " ('would', 623),\n",
       " ('really', 615),\n",
       " ('hair', 589),\n",
       " ('cookie', 585),\n",
       " ('cookies', 575),\n",
       " ('also', 543),\n",
       " ('get', 506),\n",
       " ('oatmeal', 469),\n",
       " ('chips', 462),\n",
       " ('skin', 447),\n",
       " ('flavor', 417),\n",
       " ('little', 416),\n",
       " ('try', 407),\n",
       " ('buy', 403),\n",
       " ('even', 397),\n",
       " ('much', 395),\n",
       " ('it.', 395),\n",
       " (\"i've\", 384),\n",
       " ('using', 384),\n",
       " ('used', 381),\n",
       " (\"i'm\", 373),\n",
       " ('dog', 366),\n",
       " ('tried', 354),\n",
       " ('-', 329),\n",
       " ('eat', 320),\n",
       " ('best', 317),\n",
       " ('greenies', 315),\n",
       " ('them.', 295),\n",
       " ('baked', 294),\n",
       " ('bought', 293),\n",
       " ('quaker', 293),\n",
       " ('dogs', 292),\n",
       " ('loves', 291),\n",
       " ('better', 290),\n",
       " ('first', 289),\n",
       " ('/>i', 285),\n",
       " ('loved', 282),\n",
       " ('potato', 280),\n",
       " ('definitely', 275),\n",
       " ('sweet', 273),\n",
       " ('price', 272),\n",
       " ('time', 266),\n",
       " ('got', 265),\n",
       " ('make', 265),\n",
       " ('go', 260),\n",
       " ('since', 258),\n",
       " ('many', 254),\n",
       " ('received', 253),\n",
       " ('oil.', 253),\n",
       " ('cooking', 252),\n",
       " ('teeth', 247),\n",
       " ('healthy', 246),\n",
       " ('box', 244),\n",
       " ('give', 244),\n",
       " ('&', 239),\n",
       " ('recommend', 236),\n",
       " ('think', 233),\n",
       " ('well', 232),\n",
       " ('product.', 223),\n",
       " ('know', 221),\n",
       " ('put', 221),\n",
       " ('bag', 217),\n",
       " ('two', 217),\n",
       " ('keep', 215),\n",
       " ('every', 214),\n",
       " ('snack', 212),\n",
       " ('kids', 204),\n",
       " (\"can't\", 204),\n",
       " ('made', 198),\n",
       " ('find', 197),\n",
       " ('still', 196),\n",
       " ('never', 195),\n",
       " ('say', 194),\n",
       " ('oil,', 193),\n",
       " ('bit', 192),\n",
       " ('way', 190),\n",
       " ('always', 186),\n",
       " ('buying', 184),\n",
       " ('found', 183),\n",
       " ('it,', 183),\n",
       " ('eating', 182),\n",
       " ('smells', 182),\n",
       " ('smell', 181),\n",
       " ('something', 179),\n",
       " ('nice', 178),\n",
       " ('could', 172),\n",
       " ('popchips', 172),\n",
       " ('influenster', 170),\n",
       " ('regular', 169),\n",
       " ('brand', 169),\n",
       " ('2', 168),\n",
       " ('old', 166),\n",
       " ('size', 165),\n",
       " ('salt', 165),\n",
       " ('fat', 164),\n",
       " ('feel', 164),\n",
       " ('want', 164),\n",
       " ('whole', 163),\n",
       " ('right', 163),\n",
       " ('tastes', 163),\n",
       " ('dry', 160),\n",
       " ('chips.', 160),\n",
       " ('makes', 159),\n",
       " ('raisin', 159),\n",
       " ('organic', 157),\n",
       " ('health', 155),\n",
       " ('see', 153),\n",
       " ('thing', 153),\n",
       " ('food', 152),\n",
       " ('day', 152),\n",
       " ('delicious', 152),\n",
       " ('flavor.', 152),\n",
       " ('absolutely', 151),\n",
       " ('without', 150),\n",
       " ('sure', 149),\n",
       " ('small', 146),\n",
       " ('need', 146),\n",
       " ('perfect', 145),\n",
       " ('take', 145),\n",
       " ('treat', 143),\n",
       " ('good.', 143),\n",
       " ('jar', 141),\n",
       " ('mom', 140),\n",
       " ('order', 139),\n",
       " ('lot', 138),\n",
       " ('going', 137),\n",
       " ('extra', 137),\n",
       " ('different', 137),\n",
       " ('cookies.', 137),\n",
       " ('chip', 135),\n",
       " (\"they're\", 134),\n",
       " ('almost', 133),\n",
       " ('butter', 132),\n",
       " ('big', 131),\n",
       " ('less', 131),\n",
       " ('enough', 130),\n",
       " ('sample', 129),\n",
       " ('favorite', 127)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = count_words(amz_prd['text'])\n",
    "counts.most_common(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: which stopwords to remove? / adding in custom stopwords?\n",
    "\n",
    "- A: nltk.corpus.stopwords + customized ones below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> by comparing common words with default stopwords, add more stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.393845Z",
     "start_time": "2022-04-12T22:35:42.391578Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['product', 'would', 'really', 'also', 'even', 'much', 'definitely', 'many',\n",
    "            'every', 'still', 'always', 'bit', 'way', 'something', 'whole', 'thing',\n",
    "            'absolutely', 'lot', 'almost', 'enough', 'food', 'foods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regx cleaning\n",
    "- regex cleaning and substitution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.397123Z",
     "start_time": "2022-04-12T22:35:42.395096Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_word(doc, word_orig, word_std):\n",
    "    doc = doc.str.replace(word_orig, word_std,\n",
    "                          flags=re.IGNORECASE, regex=True)\n",
    "    return doc # has to return, otherwise local var wont affect global var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.401005Z",
     "start_time": "2022-04-12T22:35:42.398761Z"
    }
   },
   "outputs": [],
   "source": [
    "amz_prd['text_std'] = amz_prd['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<br />` cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.407261Z",
     "start_time": "2022-04-12T22:35:42.401920Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'(<br />)', ''\n",
    "amz_prd['text_std'] = standardize_word(amz_prd['text_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`('-', 329)`, `('&', 239)` cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.422167Z",
     "start_time": "2022-04-12T22:35:42.408137Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'(-|&)', ''\n",
    "amz_prd['text_std'] = standardize_word(amz_prd['text_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.439117Z",
     "start_time": "2022-04-12T22:35:42.423462Z"
    }
   },
   "outputs": [],
   "source": [
    "word_orig, word_std = r'[.!?\\-\\\"\\\\]', ' '\n",
    "amz_prd['text_std'] = standardize_word(amz_prd['text_std'], word_orig, word_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:42.800502Z",
     "start_time": "2022-04-12T22:35:42.440291Z"
    }
   },
   "outputs": [],
   "source": [
    "amz_prd['text_std'] = amz_prd['text_std'].apply(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T12:52:12.467691Z",
     "start_time": "2022-04-12T12:52:12.459084Z"
    }
   },
   "source": [
    "numbers, currency_symbols, emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:43.315742Z",
     "start_time": "2022-04-12T22:35:42.805469Z"
    }
   },
   "outputs": [],
   "source": [
    "amz_prd['text_std'] = amz_prd['text_std'].\\\n",
    "                        apply(urls).\\\n",
    "                        apply(numbers).\\\n",
    "                        apply(currency_symbols).\\\n",
    "                        apply(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: stemming versus lemmatization?\n",
    "- A: after conducting lemmatization, we found that TF-IDF vectorizer identifies `oatmeal cookie, oatmeal cookies, oatmeal cooky` as three different tokens, thus we choose stemming in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:43.320608Z",
     "start_time": "2022-04-12T22:35:43.318512Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemming_sentence(sentence):\n",
    "    stemmed_sentence = []\n",
    "    for word in sentence.split(' '):      \n",
    "        stemmed_sentence.append(stemmer.stem(word))\n",
    "    return \" \".join(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.053045Z",
     "start_time": "2022-04-12T22:35:43.321791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3291/3291 [00:03<00:00, 882.91it/s] \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "for i in tqdm(amz_prd.index):\n",
    "    amz_prd.loc[i, 'text_std'] = stemming_sentence(amz_prd.loc[i, 'text_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after preliminary vectorization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.056074Z",
     "start_time": "2022-04-12T22:35:47.054058Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['_cur_', '_number_', 'star', 'stars', 'able', 'actually', 'ago', 'already',\n",
    "            'although', 'another', 'anyone', 'www', 'com', 'gp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add stpw after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.058483Z",
     "start_time": "2022-04-12T22:35:47.057047Z"
    }
   },
   "outputs": [],
   "source": [
    "stpw |= set(['thi', 'absolut', 'wa', 'ver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: what n for your n-grams?\n",
    "- A: we pick `ngram_range=(2,3)` as our hyperparameter. As the reviews are considerably lengthy, we choose `ngram_range=(2,3)` to better represent the pattern of reviews. Also, by choosing `ngram_range=(2,3)`, we can get a basic idea of products and customers' opinions in two and three words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.353870Z",
     "start_time": "2022-04-12T22:35:47.059463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (3291, 125)\n",
      "Total number of occurences: 8541\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bake cooki</th>\n",
       "      <th>bake oatmeal</th>\n",
       "      <th>bake oatmeal cooki</th>\n",
       "      <th>best price</th>\n",
       "      <th>coconut flavor</th>\n",
       "      <th>coconut oil</th>\n",
       "      <th>coconut oil ha</th>\n",
       "      <th>coconut oil use</th>\n",
       "      <th>coconut tast</th>\n",
       "      <th>cook oil</th>\n",
       "      <th>...</th>\n",
       "      <th>use oil</th>\n",
       "      <th>use skin</th>\n",
       "      <th>veri good</th>\n",
       "      <th>veri happi</th>\n",
       "      <th>veri soft</th>\n",
       "      <th>veri tasti</th>\n",
       "      <th>virgin coconut</th>\n",
       "      <th>virgin coconut oil</th>\n",
       "      <th>work great</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bake cooki  bake oatmeal  bake oatmeal cooki  best price  coconut flavor  \\\n",
       "0           0             0                   0           0               0   \n",
       "1           0             0                   0           1               0   \n",
       "2           0             0                   0           0               0   \n",
       "3           0             0                   0           0               0   \n",
       "4           0             0                   0           0               0   \n",
       "\n",
       "   coconut oil  coconut oil ha  coconut oil use  coconut tast  cook oil  ...  \\\n",
       "0            0               0                0             0         0  ...   \n",
       "1            0               0                0             0         0  ...   \n",
       "2            0               0                0             0         0  ...   \n",
       "3            0               0                0             0         0  ...   \n",
       "4            0               0                0             0         0  ...   \n",
       "\n",
       "   use oil  use skin  veri good  veri happi  veri soft  veri tasti  \\\n",
       "0        0         0          0           0          0           0   \n",
       "1        0         0          0           0          0           0   \n",
       "2        0         0          0           0          0           0   \n",
       "3        0         0          0           0          0           0   \n",
       "4        0         0          0           0          0           0   \n",
       "\n",
       "   virgin coconut  virgin coconut oil  work great  year old  \n",
       "0               0                   0           0         1  \n",
       "1               0                   0           0         0  \n",
       "2               0                   0           0         0  \n",
       "3               0                   0           0         0  \n",
       "4               0                   0           0         0  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), stop_words=stpw, binary=True, min_df=0.01)\n",
    "X = vectorizer.fit_transform(amz_prd['text_std'])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.358357Z",
     "start_time": "2022-04-12T22:35:47.354913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bake cooki\n",
      "bake oatmeal\n",
      "bake oatmeal cooki\n",
      "best price\n",
      "coconut flavor\n",
      "coconut oil\n",
      "coconut oil ha\n",
      "coconut oil use\n",
      "coconut tast\n",
      "cook oil\n",
      "cook use\n",
      "cooki delici\n",
      "cooki good\n",
      "cooki great\n",
      "cooki influenst\n",
      "cooki raisin\n",
      "cooki soft\n",
      "cooki tast\n",
      "cooki veri\n",
      "definit buy\n",
      "dog love\n",
      "dog love greeni\n",
      "dog teeth\n",
      "dri skin\n",
      "everi day\n",
      "extra virgin\n",
      "extra virgin coconut\n",
      "feel like\n",
      "first time\n",
      "get one\n",
      "give one\n",
      "give tri\n",
      "go buy\n",
      "good price\n",
      "good tast\n",
      "great price\n",
      "great snack\n",
      "great tast\n",
      "hair skin\n",
      "health benefit\n",
      "help keep\n",
      "hi teeth\n",
      "highli recommend\n",
      "href http\n",
      "individu wrap\n",
      "influenst mom\n",
      "kid love\n",
      "like coconut\n",
      "look forward\n",
      "love coconut\n",
      "love coconut oil\n",
      "love cooki\n",
      "love greeni\n",
      "love love\n",
      "love oatmeal\n",
      "love soft\n",
      "love tast\n",
      "love use\n",
      "mani use\n",
      "mom voxbox\n",
      "nutiva organ\n",
      "oatmeal cooki\n",
      "oatmeal raisin\n",
      "oatmeal raisin cooki\n",
      "oil cook\n",
      "oil great\n",
      "oil ha\n",
      "oil use\n",
      "oliv oil\n",
      "one day\n",
      "organ coconut\n",
      "organ coconut oil\n",
      "organ extra\n",
      "organ extra virgin\n",
      "pet store\n",
      "pleasantli surpris\n",
      "pop chip\n",
      "potato chip\n",
      "quaker soft\n",
      "quaker soft bake\n",
      "raisin cooki\n",
      "realli enjoy\n",
      "realli good\n",
      "realli like\n",
      "receiv sampl\n",
      "regular chip\n",
      "salt pepper\n",
      "salt vinegar\n",
      "sea salt\n",
      "skin hair\n",
      "smell good\n",
      "smell great\n",
      "smell like\n",
      "soft bake\n",
      "soft bake cooki\n",
      "soft bake oatmeal\n",
      "soft chewi\n",
      "soft cooki\n",
      "start use\n",
      "subscrib save\n",
      "sweet potato\n",
      "tast better\n",
      "tast coconut\n",
      "tast good\n",
      "tast great\n",
      "tast like\n",
      "tast veri\n",
      "teeth clean\n",
      "tri flavor\n",
      "use coconut\n",
      "use coconut oil\n",
      "use cook\n",
      "use hair\n",
      "use make\n",
      "use moistur\n",
      "use oil\n",
      "use skin\n",
      "veri good\n",
      "veri happi\n",
      "veri soft\n",
      "veri tasti\n",
      "virgin coconut\n",
      "virgin coconut oil\n",
      "work great\n",
      "year old\n"
     ]
    }
   ],
   "source": [
    "for i in vectorized_df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### poor review features\n",
    "the features your analysis showed that customers cited as reasons for a poor review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:40.504477Z",
     "start_time": "2022-04-12T22:37:40.494350Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_poor = amz_prd.loc[amz_prd['score']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:40:56.506729Z",
     "start_time": "2022-04-12T22:40:56.469557Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(2,3), stop_words=stpw, binary=True, min_df=0.01)\n",
    "\n",
    "X = tfidf_vec.fit_transform(rev_poor['text_std'])\n",
    "terms = tfidf_vec.get_feature_names_out()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### most common issues of customer dissatisfaction\n",
    "the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "*You should explain what the pain points are and cite specific examples/action items from the corpus for management to consider*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:40:57.443405Z",
     "start_time": "2022-04-12T22:40:57.430949Z"
    }
   },
   "outputs": [],
   "source": [
    "score = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(score, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:40:57.936263Z",
     "start_time": "2022-04-12T22:40:57.931616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>potato chip</th>\n",
       "      <td>4.260429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut oil</th>\n",
       "      <td>4.197570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salt vinegar</th>\n",
       "      <td>4.025182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri disappoint</th>\n",
       "      <td>2.303440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salt pepper</th>\n",
       "      <td>2.123748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack materi</th>\n",
       "      <td>2.051615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal cooki</th>\n",
       "      <td>2.016355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greeni dog</th>\n",
       "      <td>1.983555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri one</th>\n",
       "      <td>1.940954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast like</th>\n",
       "      <td>1.908749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop chip</th>\n",
       "      <td>1.864022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throw away</th>\n",
       "      <td>1.788240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digest tract</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anoth box</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go buy</th>\n",
       "      <td>1.664458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    score\n",
       "potato chip      4.260429\n",
       "coconut oil      4.197570\n",
       "salt vinegar     4.025182\n",
       "veri disappoint  2.303440\n",
       "salt pepper      2.123748\n",
       "pack materi      2.051615\n",
       "oatmeal cooki    2.016355\n",
       "greeni dog       1.983555\n",
       "tri one          1.940954\n",
       "tast like        1.908749\n",
       "pop chip         1.864022\n",
       "throw away       1.788240\n",
       "digest tract     1.707107\n",
       "anoth box        1.707107\n",
       "go buy           1.664458"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> most common issues of customer dissatisfaction\n",
    "\n",
    "**coconut oil**:\n",
    "the following reviews like\n",
    "`This is not cocnut oil at all. It is more like dalda(vanaspati)in cosistency and has no coconut flavor/smell at all.`, \n",
    "`It's not USDA approved and the scent smelled artifical. The texture was heavier then most unrefined coconut oil.`,\n",
    "`there is cocunut oil all over the 2 jars, bubble wrap and all over the box!` \n",
    " demonstrate that the product quality and packaging cause the dissatisfaction and management should consider to choose good-brand ones and require extra attention to the pacakaging process for the oil-like product\n",
    " \n",
    "**taste like**:\n",
    "reviews like `They really do taste like dissolving cardboard`, `made the weak coffee taste like apple peelings`, \n",
    "`it does not taste like Cappuccino at all`, `It does not really taste like a cup of cappuchino` demonstrate that the food taste is the key issue for customer dissatisfaction. Therefore, management should increse the standard for food quality, especially its taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### good review features\n",
    "the features your analysis showed that customers cited as reasons for a good review (i.e. 4/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.398014Z",
     "start_time": "2022-04-12T22:35:47.395704Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_good = amz_prd.loc[amz_prd['score']>=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.657163Z",
     "start_time": "2022-04-12T22:35:47.399010Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(2,3), stop_words=stpw, binary=True, min_df=0.01)\n",
    "\n",
    "X = tfidf_vec.fit_transform(rev_good['text_std'])\n",
    "terms = tfidf_vec.get_feature_names_out()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.667762Z",
     "start_time": "2022-04-12T22:35:47.658259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coconut oil</th>\n",
       "      <td>229.715145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog love</th>\n",
       "      <td>99.742412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal cooki</th>\n",
       "      <td>73.247019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potato chip</th>\n",
       "      <td>68.680427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highli recommend</th>\n",
       "      <td>65.091544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast like</th>\n",
       "      <td>62.167670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love cooki</th>\n",
       "      <td>58.595070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use cook</th>\n",
       "      <td>57.601552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use hair</th>\n",
       "      <td>56.038366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use coconut</th>\n",
       "      <td>55.808612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love greeni</th>\n",
       "      <td>55.264270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast great</th>\n",
       "      <td>55.104534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use coconut oil</th>\n",
       "      <td>54.843559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft chewi</th>\n",
       "      <td>54.260259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri good</th>\n",
       "      <td>53.250111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "coconut oil       229.715145\n",
       "dog love           99.742412\n",
       "oatmeal cooki      73.247019\n",
       "potato chip        68.680427\n",
       "highli recommend   65.091544\n",
       "tast like          62.167670\n",
       "love cooki         58.595070\n",
       "use cook           57.601552\n",
       "use hair           56.038366\n",
       "use coconut        55.808612\n",
       "love greeni        55.264270\n",
       "tast great         55.104534\n",
       "use coconut oil    54.843559\n",
       "soft chewi         54.260259\n",
       "veri good          53.250111"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(score, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "score.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pros and cons\n",
    "- Q: Explain to what degree the TF-IDF findings make sense - what are its limitations?\n",
    "- A: TF-IDF quantifies the importance of words, terms and so on and help to exact the most descriptive terms. However, TF-IDF can not help carry semantic meaning which means TF-IDF can not consider semantically similar words together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both **Euclidean distance** and **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:47.674609Z",
     "start_time": "2022-04-12T22:35:47.669544Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_most_similar_pair(data: pd.DataFrame, metric='euclidean_distance') -> pd.DataFrame:\n",
    "    \n",
    "    if metric=='euclidean_distance':\n",
    "        metric_df = pd.DataFrame(euclidean_distances(data), \n",
    "                                 index=amz_prd['id'], columns=amz_prd['id'])\n",
    "    elif metric=='cosine_similarity':\n",
    "        metric_df = pd.DataFrame(cosine_similarity(data), \n",
    "                                 index=amz_prd['id'], columns=amz_prd['id'])\n",
    "        metric_df = metric_df.where(metric_df<1., 1.)\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    metric_df = metric_df.where(np.triu(np.ones(metric_df.shape), k=1).astype(bool))\n",
    "    optimum = metric_df.min().min() if metric=='euclidean_distance' else metric_df.max().max()\n",
    "\n",
    "    pair = metric_df.stack().rename_axis(('doc1_id','doc2_id')).reset_index(name='value')\n",
    "    pair = pair.loc[pair['value']==optimum]\n",
    "    pair['text1'], pair['text2'] = np.nan, np.nan\n",
    "    for i in pair.index:\n",
    "        pair.loc[i, 'text1'] = amz_prd.loc[amz_prd['id']==pair.loc[i, 'doc1_id'], 'text'].values\n",
    "        pair.loc[i, 'text2'] = amz_prd.loc[amz_prd['id']==pair.loc[i, 'doc2_id'], 'text'].values\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove restrictions like `min_df=0.01` to increase the dim of vector and thus increase the liability of the distance/similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:35:48.537039Z",
     "start_time": "2022-04-12T22:35:47.675850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 166666)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(2,3), stop_words=stpw, binary=True)\n",
    "\n",
    "X = tfidf_vec.fit_transform(amz_prd['text'])\n",
    "terms = tfidf_vec.get_feature_names_out()\n",
    "tf_idf = pd.DataFrame(X.toarray(), columns=terms, index=amz_prd['id'])\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:36:09.816357Z",
     "start_time": "2022-04-12T22:35:48.538026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1_id</th>\n",
       "      <th>doc2_id</th>\n",
       "      <th>value</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335967</th>\n",
       "      <td>189401</td>\n",
       "      <td>189415</td>\n",
       "      <td>0.267027</td>\n",
       "      <td>I am happy to say that I found the texture in ...</td>\n",
       "      <td>I received a mini popcorn machine like you wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc1_id  doc2_id     value  \\\n",
       "3335967   189401   189415  0.267027   \n",
       "\n",
       "                                                     text1  \\\n",
       "3335967  I am happy to say that I found the texture in ...   \n",
       "\n",
       "                                                     text2  \n",
       "3335967  I received a mini popcorn machine like you wou...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_eucl = find_most_similar_pair(tf_idf, metric='euclidean_distance')\n",
    "pair_eucl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:36:40.267198Z",
     "start_time": "2022-04-12T22:36:09.824299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1_id</th>\n",
       "      <th>doc2_id</th>\n",
       "      <th>value</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335967</th>\n",
       "      <td>189401</td>\n",
       "      <td>189415</td>\n",
       "      <td>0.964348</td>\n",
       "      <td>I am happy to say that I found the texture in ...</td>\n",
       "      <td>I received a mini popcorn machine like you wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc1_id  doc2_id     value  \\\n",
       "3335967   189401   189415  0.964348   \n",
       "\n",
       "                                                     text1  \\\n",
       "3335967  I am happy to say that I found the texture in ...   \n",
       "\n",
       "                                                     text2  \n",
       "3335967  I received a mini popcorn machine like you wou...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_cos = find_most_similar_pair(tf_idf, metric='cosine_similarity')\n",
    "pair_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:36:40.947835Z",
     "start_time": "2022-04-12T22:36:40.269260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 161232)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(2,3), stop_words=stpw, binary=True)\n",
    "\n",
    "X = tfidf_vec.fit_transform(amz_prd['text_std'])\n",
    "terms = tfidf_vec.get_feature_names_out()\n",
    "tf_idf = pd.DataFrame(X.toarray(), columns=terms, index=amz_prd['id'])\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:01.867154Z",
     "start_time": "2022-04-12T22:36:40.949365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1_id</th>\n",
       "      <th>doc2_id</th>\n",
       "      <th>value</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335967</th>\n",
       "      <td>189401</td>\n",
       "      <td>189415</td>\n",
       "      <td>0.24347</td>\n",
       "      <td>I am happy to say that I found the texture in ...</td>\n",
       "      <td>I received a mini popcorn machine like you wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc1_id  doc2_id    value  \\\n",
       "3335967   189401   189415  0.24347   \n",
       "\n",
       "                                                     text1  \\\n",
       "3335967  I am happy to say that I found the texture in ...   \n",
       "\n",
       "                                                     text2  \n",
       "3335967  I received a mini popcorn machine like you wou...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_eucl = find_most_similar_pair(tf_idf, metric='euclidean_distance')\n",
    "pair_eucl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.449569Z",
     "start_time": "2022-04-12T22:37:01.869169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1_id</th>\n",
       "      <th>doc2_id</th>\n",
       "      <th>value</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335967</th>\n",
       "      <td>189401</td>\n",
       "      <td>189415</td>\n",
       "      <td>0.970361</td>\n",
       "      <td>I am happy to say that I found the texture in ...</td>\n",
       "      <td>I received a mini popcorn machine like you wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc1_id  doc2_id     value  \\\n",
       "3335967   189401   189415  0.970361   \n",
       "\n",
       "                                                     text1  \\\n",
       "3335967  I am happy to say that I found the texture in ...   \n",
       "\n",
       "                                                     text2  \n",
       "3335967  I received a mini popcorn machine like you wou...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_cos = find_most_similar_pair(tf_idf, metric='cosine_similarity')\n",
    "pair_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all four evaluations, the following reviews are the most similar pair:\n",
    "    \n",
    "    I am happy to say that I found the texture in the jar to be harder then I thought (though it may have been a bit frozen from outside in the box). It definitely smells like coconut, like a mounds bar. I was so concerned about the popcorn tasting like nothing but coconut. I was VERY surprised buy the wonderful taste it gives the popcorn, I can't believe I never realized at the movies that the reason the popcorn taste so good is that bit of coconut flavor to it. It also left my popcorn machine very clean, when the other oil coated the whole machine in a greasy gunk. Combine with Eden Organics Organic popcorn kernels from Amazon for a total organic popcorn. I received a mini popcorn machine like you would find in a theater for Christmas. After going through the sample packs of yellow gunk oil that came with it for popping I searched for a more healthier and natural way to make popcorn. I read a lot about different oils people use, but one thing was always the same, if you want it to taste like at the movies use coconut oil.\n",
    "    \n",
    "    ----\n",
    "    \n",
    "    I received a mini popcorn machine like you would find in a theater for Christmas.  After going through the sample packs of yellow gunk oil that came with it for popping I searched for a more healthier and natural way to make popcorn.  I read a lot about different oils people use, but one thing was always the same, if you want it to taste like at the movies use coconut oil.<br /><br />I found the texture in the jar to be harder then I thought (though it may have been a bit frozen from outside in the box).  It definitely smells like coconut, like a mounds bar.  I was so concerned about the popcorn tasting like nothing but coconut.  I was VERY surprised buy the wonderful taste it gives the popcorn, I can't believe I never realized at the movies that the reason the popcorn taste so good is that bit of coconut flavor to it.  It also left my popcorn machine very clean, when the other oil coated the whole machine in a greasy gunk. Combine with Eden Organics Organic popcorn kernels from Amazon for a total organic popcorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handwriten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***remove stopwords & stemming***:\n",
    "\n",
    "**yes $:=$ Intent to Buy Tickets:**\n",
    "1.  love movie cannot wait\n",
    "2.  I want see movie so bad\n",
    "3.  movie look amazing\n",
    "\n",
    "**no $:=$ No Intent to Buy Tickets:**\n",
    "1.\tlook bad\n",
    "2.\thard pass see bad movie\n",
    "3.\tso boring\n",
    "\n",
    "**test text:**\n",
    "look so bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(y=yes) = \\frac{3}{3+3} = 0.5 \\\\\n",
    "P(y=no) = \\frac{3}{3+3} = 0.5\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(X|y=yes) &= P(x=look|y=yes) * P(x=so|y=yes) * P(x=bad|y=yes) \\\\\n",
    "& = \\frac{1}{3} * \\frac{1}{3} * \\frac{1}{3} \\\\\n",
    "& = \\frac{1}{27}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(X|y=no) &= P(x=look|y=no) * P(x=so|y=no) * P(x=bad|y=no) \\\\\n",
    "& = \\frac{1}{3} * \\frac{1}{3} * \\frac{2}{3} \\\\\n",
    "& = \\frac{2}{27}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(y=yes|X) &\\propto P(X|y=yes) * P(y=yes) \\\\\n",
    "& = \\frac{1}{27} * \\frac{1}{2} \\\\\n",
    "& = \\frac{1}{54}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(y=no|X) &\\propto P(X|y=no) * P(y=no) \\\\\n",
    "& = \\frac{2}{27} * \\frac{1}{2} \\\\\n",
    "& = \\frac{1}{27}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $P(y=no|X) > P(y=yes|X)$, this review is considered as `No Intent to Buy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating $P(X)$ to get real postrior:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(y=yes|X) &= \\frac{P(X|y=yes) * P(y=yes)}{P(X)} \\\\\n",
    "& = \\frac{\\frac{1}{54}}{\\frac{1}{18}} \\\\\n",
    "& = \\frac{1}{3}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "P(y=no|X) &= \\frac{P(X|y=no) * P(y=no)}{P(X)} \\\\\n",
    "& = \\frac{\\frac{1}{27}}{\\frac{1}{18}} \\\\\n",
    "& = \\frac{2}{3}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.455380Z",
     "start_time": "2022-04-12T22:37:31.451569Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove(text):\n",
    "    text=text.lower()\n",
    "    sw=['to','this']\n",
    "    words = text.split()\n",
    "    sentence = []\n",
    "    for i in words:\n",
    "        if i in sw:\n",
    "            continue\n",
    "        sentence.append(i)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.461520Z",
     "start_time": "2022-04-12T22:37:31.457284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"love movie. can't wait\", 'BUY'),\n",
       " ('i want see movie so bad', 'BUY'),\n",
       " ('movie looks amazing', 'BUY'),\n",
       " ('looks bad', 'NOT_BUY'),\n",
       " ('hard pass see bad movie', 'NOT_BUY'),\n",
       " ('so boring', 'NOT_BUY')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    (remove(\"Love this movie. Can't wait\"), \"BUY\"),\n",
    "    (remove(\"I want to see this movie so bad\"), \"BUY\"),\n",
    "    (remove(\"This movie looks amazing\"), \"BUY\"),  \n",
    "    (remove(\"Looks bad\"), \"NOT_BUY\"),\n",
    "    (remove(\"Hard pass to see this bad movie\"), \"NOT_BUY\"),\n",
    "    (remove(\"So boring\"), \"NOT_BUY\")\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.464571Z",
     "start_time": "2022-04-12T22:37:31.462536Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.469404Z",
     "start_time": "2022-04-12T22:37:31.465929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amazing',\n",
       " 'bad',\n",
       " 'boring',\n",
       " \"can't\",\n",
       " 'hard',\n",
       " 'i',\n",
       " 'looks',\n",
       " 'love',\n",
       " 'movie',\n",
       " 'movie.',\n",
       " 'pass',\n",
       " 'see',\n",
       " 'so',\n",
       " 'wait',\n",
       " 'want'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        corpus.add(word)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.475709Z",
     "start_time": "2022-04-12T22:37:31.470499Z"
    }
   },
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_buy\", \"likelihood_given_not_buy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.479243Z",
     "start_time": "2022-04-12T22:37:31.476807Z"
    }
   },
   "outputs": [],
   "source": [
    "buy_documents = 0\n",
    "not_buy_documents = 0\n",
    "for document in documents:\n",
    "    if document[1] == \"BUY\":\n",
    "        buy_documents += 1\n",
    "    else:\n",
    "        not_buy_documents += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.483838Z",
     "start_time": "2022-04-12T22:37:31.480445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_buy = buy_documents / (buy_documents + not_buy_documents)\n",
    "p_not_buy = not_buy_documents / (buy_documents + not_buy_documents)\n",
    "p_buy, p_not_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.489406Z",
     "start_time": "2022-04-12T22:37:31.484997Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in corpus:\n",
    "    buy_documents_with_word = 0\n",
    "    not_buy_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"BUY\":\n",
    "                buy_documents_with_word += 1\n",
    "            else:\n",
    "                not_buy_documents_with_word += 1\n",
    "    \n",
    "    conditional_probabilities.loc[word, \"likelihood_given_buy\"] = buy_documents_with_word * 1.0 / buy_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_not_buy\"] = not_buy_documents_with_word * 1.0 / not_buy_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.492969Z",
     "start_time": "2022-04-12T22:37:31.491164Z"
    }
   },
   "outputs": [],
   "source": [
    "test_document = remove(\"This looks so bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.496246Z",
     "start_time": "2022-04-12T22:37:31.493853Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_likelihood(test_document, conditional_probabilities):\n",
    "    likelihood_buy = 1\n",
    "    likelihood_not_buy = 1\n",
    "    for word in test_document.split():\n",
    "        likelihood_buy = likelihood_buy * conditional_probabilities.loc[word, \"likelihood_given_buy\"]\n",
    "        likelihood_not_buy = likelihood_not_buy * conditional_probabilities.loc[word, \"likelihood_given_not_buy\"]\n",
    "    \n",
    "    return likelihood_buy, likelihood_not_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.500488Z",
     "start_time": "2022-04-12T22:37:31.497439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.037037037037037035, 0.07407407407407407)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_buy, likelihood_not_buy = get_likelihood(test_document, conditional_probabilities)\n",
    "likelihood_buy, likelihood_not_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.503639Z",
     "start_time": "2022-04-12T22:37:31.501530Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_buy, likelihood_not_buy, p_buy, p_not_buy):\n",
    "    posterior_buy = likelihood_buy * p_buy / (likelihood_buy * p_buy + likelihood_not_buy * p_not_buy)\n",
    "    posterior_not_buy = likelihood_not_buy * p_not_buy / (likelihood_buy * p_buy + likelihood_not_buy * p_not_buy)\n",
    "    return posterior_buy, posterior_not_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:37:31.507636Z",
     "start_time": "2022-04-12T22:37:31.504931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.6666666666666666)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posterior(likelihood_buy, likelihood_not_buy, p_buy, p_not_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
